\section{Continuous time, theoretical motivation }

The purpose of this project is to approximate $\expt{g(\mathbf{X}_T)}$, where $g$ is a certain payoff function and $\mathbf{X}=(X_1, \dots, X_d)$ is described by the following SDE

\begin{equation}\label{eq:SDE_interest}
dX_i=a_i(X) dt + \sum_{j=1}^d b_{ij}(X) dW_j \PERIOD
\end{equation}
Without  loss of generality, we assume that the $\{W_j\}_{j=1}^d$ are uncorrelated (the correlation terms can be included the diffusion terms $b_j$).

First, we start by representing hierarchically $\mathbf{W}$. In fact, we can write 

\begin{align}
W_j(t)&= \frac{t}{T} W_j(T)+B_j(t) \nonumber\\
&= \frac{t}{\sqrt{T}} Y_j+B_j(t)\COMMA
\end{align}
with $Y_j \sim \mathcal{N}(0,1)$ iid and $\{B_j\}_{j=1}^d$ are the Brownian bridges.

Now, we aim to represent $\mathbf{Y}=(Y_1,\dots,Y_d)$ hierarchically by using a discrete Brownian bridge (Bb) namely

\begin{equation}\label{eq:smoothing_decomposition}
\mathbf{Y}= \underset{\text{One dimensional projection}}{\underbrace{\mathbb{P}_0 \mathbf{Y}}} +  \underset{\text{Projection on the complementary}} {\underbrace{\mathbb{P}_{\perp} \mathbf{Y}}}\COMMA
\end{equation} 
where we write $\mathbb{P}_0 \mathbf{Y}=(\mathbf{Y}, \mathbf{v}) \mathbf{v}$, with $(.,.)$ denotes the scalar product and $\mid \mid \mathbf{v} \mid \mid=1$. We can easily show that $Z_v:=(Y,v)$ is normal with $\expt{Z_v}=0$ and $\text{Var}(Z_v)=1$. Furthermore, we can write
\begin{align}\label{eq:smoothing_decomposition_componentwise}
Y_j&=Z_v v_j+ (\mathbb{P}_{\perp} \mathbf{Y})_j\nonumber \\
&=Z_v v_j+ (Z_v^\perp)_j \PERIOD
\end{align}

The first aim of the project is to determine the optimal direction $\mathbf{v}$. By optimal direction, we mean the direction that maximizes the smoothing effect, that is the one given by

\begin{align}\label{optimality_criterion}
\underset{\underset{\mid \mid \mathbf{v}\mid \mid=1} {\mathbf{v} \in \rset^{d \times 1}}}{\operatorname{\sup}} \left(\underset{1 \le i \le d}{\max}\abs{ \frac{\partial^2 g}{\partial^2 v_i}} \right) \PERIOD
\end{align}

\red{We need to check if it is better to consider the second derivative as a metric for optimality or not. Also, one may check if formulation  \eqref{optimality_criterion} is correct. For instance, one may consider also the following metric

\begin{align}\label{optimality_criterion_2}
\underset{\underset{\mid \mid \mathbf{v}\mid \mid=1} {\mathbf{v} \in \rset^{d \times 1}}}{\operatorname{\sup}} \left(\text{Var}\left[ g(\hat{X}_T) \right] \right) \COMMA
\end{align}
where $\hat{X}$ is defined in \eqref{eq:approximate_dynamics}.
}

Going back to the SDE \eqref{eq:SDE_interest}, we have 
\begin{equation}
dX_i=a_i(X) dt+\sum_{j=1}^d b_{ij}(X) Y_j \frac{dt}{\sqrt{T}} +\sum_{j=1}^d b_{ij}(X) Y_j \frac{dt}{\sqrt{T}}+\sum_{j=1}^d b_{ij}(X) dB_j \PERIOD
\end{equation}

Using \eqref{eq:smoothing_decomposition_componentwise}, then we have

\begin{equation}\label{eq:SDE_decomposition_componentwise_exapanded}
dX_i=\left(a_i(X)+\sum_{j=1}^d b_{ij}(X)  \frac{Z_v v_j}{\sqrt{T}}\right) dt+\left(\sum_{j=1}^d b_{ij}(X) \frac{(Z_v^\perp)_j}{\sqrt{T}}\right) dt +\sum_{j=1}^d b_{ij}(X) dB_j \PERIOD
\end{equation}

\subsection{First approach (Brutal)}

We assume that the first term in the right-hand side of \eqref{eq:SDE_decomposition_componentwise_exapanded}  is the dominant term compared to the remaining terms (\red{We need to formulate clearly what we mean by this statement. Also I see some issues with this  assumption, since it implicitly implies from \eqref{eq:smoothing_decomposition_componentwise} that 
$Y \approx Z_v v$, which is already a strong assumption about the solution $v$.}), and denote the approximate process $\hat{X}$, whose dynamics are given by
\begin{align}\label{eq:approximate_dynamics}
\frac{d\hat{X}_i}{dt}=a_i(\hat{X})+\sum_{j=1}^d b_{ij}(\hat{X})  \frac{Z_v v_j}{\sqrt{T}}, \quad 1 \le i \le d \COMMA
\end{align}
with mean 

\begin{equation*}
\frac{d\expt{\hat{X}_i}}{dt}=\expt{a_i(\hat{X})}+\sum_{j=1}^d \expt{b_{ij}(\hat{X})  \frac{Z_v v_j}{\sqrt{T}}}, \quad 1 \le i \le d \COMMA
\end{equation*}
and second moment
\begin{align*}
\frac{d\expt{\hat{X}^2_i}}{dt}&=2 \expt{\hat{X}_i \frac{d\hat{X}_i}{dt}}\nonumber\\
&=2 \expt{\hat{X}_i a_i(\hat{X})}+\sum_{j=1}^d \expt{ \hat{X}_i b_{ij}(\hat{X})  \frac{Z_v v_j}{\sqrt{T}}}, \quad 1 \le i \le d \PERIOD
\end{align*}
Now, let us expand $\hat{X}$ around $Z_v$, that is

\begin{align}
\hat{X}_{(Z_v)}=\hat{X}_{(0)}+\hat{X}^\prime_{(0)} Z_v+\dots
\end{align}
Then, we have
\begin{align}\label{eq:first_moment_approximation}
\frac{d\expt{\hat{X}_i}}{dt}&\approx \expt{a_i(\hat{X}_{(0)})}+\sum_{j=1}^d \expt{b_{ij}(\hat{X}_{(0)})  \frac{Z_v v_j}{\sqrt{T}}}, \quad 1 \le i \le d\COMMA \nonumber\\ 
&=a_i(\hat{X}_{(0)})+\sum_{j=1}^d b_{ij}(\hat{X}_{(0)})  \frac{\expt{Z_v} v_j}{\sqrt{T}}, \quad 1 \le i \le d\COMMA\nonumber\\ 
&= a_i(\hat{X}_{(0)}), \quad 1 \le i \le d\PERIOD
\end{align} 

Similarly, if we approximate the second moment, we get
\begin{align}\label{eq:second_moment_approximation}
\frac{d\expt{\hat{X}^2_i}}{dt}&\approx 2 \expt{\hat{X}_{i,(0)} a_i(\hat{X}_{(0)})}+\sum_{j=1}^d \expt{ \hat{X}_{i,(0)} b_{ij}(\hat{X}_{(0)})  \frac{Z_v v_j}{\sqrt{T}}}, \quad 1 \le i \le d \COMMA \nonumber\\
&\approx 2 \hat{X}_{i,(0)} a_i(\hat{X}_{(0)})+\sum_{j=1}^d \hat{X}_{i,(0)} b_{ij}(\hat{X}_{(0)})  \frac{\expt{ Z_v} v_j}{\sqrt{T}}, \quad 1 \le i \le d \COMMA \nonumber\\
&\approx 2 \hat{X}_{i,(0)} a_i(\hat{X}_{(0)}), \quad 1 \le i \le d \PERIOD
\end{align}
The idea then is maximize, at the final time, the smoothing effect, represented by $\text{Var}\left[ g(\hat{X}_T) \right]$, where $\hat{X}_T \sim \mathcal{N}(\mu_T, \Sigma_T)$, such that $\mu_T, \Sigma_T$ are computed using   \eqref{eq:first_moment_approximation} and \eqref{eq:second_moment_approximation}.

\red{There is an issue that I see here! In fact, if we just use  the zero order expansion of $\hat{X}$ to get \eqref{eq:first_moment_approximation} and \eqref{eq:second_moment_approximation}, then we end up with ODEs that does not depend on $v$. On the other hand, to get dependence with respect to $v$ we need to push the expansion  of $\hat{X}$ at least up to the first order which will involve $\hat{X}^{\prime }$, and which can make the approach complicated for instance if one consider Heston model for example.}


\section{Discrete time, practical motivation }
To motivate our purposes, we consider the basket option under multi-dimensional GBM model where the process $\mathbf{X}$ is the discretized $d$-dimensional Black-Scholes model and the payoff function $g$ is given by
\begin{align}
	g(\mathbf{X}(T))=\max\left(\sum_{j=1}^{d} c_{j} X^{(j)}(T)-K,0  \right)	\PERIOD
\end{align}
Precisely, we are interested in the  $d$-dimensional lognormal example where the dynamics of the stock are given by
\begin{align}\label{lognormal_dynamics_basket}
	dX^{(j)}_t=\sigma^{(j)} X^{(j)}_t dW^{(j)}_t,
\end{align}
where $\{W^{(1)}, \dots,W^{(d)}\}$ are correlated Brownian motions with correlations $\rho_{ij}$.


We denote by $(Z_1^{(j)},\dots,Z_N^{(j)})$ the $N$ Gaussian independent rdvs that will be used to construct the path of the $j$-th asset $\bar{X}^{(j)}$, where $1 \le j \le d$ ($d$ denotes the number of underlyings considered in the basket). We denote  $\psi^{(j)}: (Z_1^{(j)},\dots,Z_N^{(j)}) \rightarrow (B_1^{(j)},\dots,B_N^{(j)})$ the mapping of Brownian bridge construction and by $\Phi: (\Delta t, \widetilde{B}^{(1)}_1,\dots,\widetilde{B}^{(1)}_N,\dots, \widetilde{B}^{(d)}_1,\dots,\widetilde{B}^{(d)}_N) \rightarrow \left(\bar{X}^{(1)}_T,\dots,\bar{X}^{(d)}_T \right)$, the mapping consisting of the time-stepping scheme, where $\widetilde{\mathbf{B}}$ is the correlated Brownian bridge that can be obtained from the non correlated Brownian bridge $\mathbf{B}$ through multiplication by the correlation matrix, we denote this transformation by $T: \left(B^{(1)}_1,\dots,B^{(1)}_N,\dots, B^{(d)}_1,\dots,B^{(d)}_N \right) \rightarrow \left(\widetilde{B}^{(1)}_1,\dots,\widetilde{B}^{(1)}_N,\dots, \widetilde{B}^{(d)}_1,\dots,\widetilde{B}^{(d)}_N\right)$. Then, we can express the option price as
\begin{align}\label{eq: option price as integral_basket}
	\expt{g(\mathbf{X}(T))}&\approx	\expt{g\left(\bar{X}_T^{(1)}, \dots,\bar{X}_T^{(d)} \right)} \nonumber\\
	&=\expt{g\left(\Phi\circ T\right) \left(B^{(1)}_1,\dots,B^{(1)}_N,\dots, B^{(d)}_1,\dots,B^{(d)}_N \right)} \nonumber\\
		&=\expt{g\left(\Phi \circ T \right) \left(\psi^{(1)}(Z_1^{(1)}, \dots, Z_N^{(1)}), \dots, \psi^{(d)}(Z_1^{(d)},\dots,Z^{(d)}_N)\right)} \nonumber\\
	&=\int_{\rset^{d \times N}} G(z_1^{(1)}, \dots, z_N^{(1)}, \dots, z_1^{(d)},\dots,z^{(d)}_N)) \rho_{d \times N}(\mathbf{z}) dz_1^{(1)} \dots dz_N^{(1)} \dots z_1^{(d)} \dots dz^{(d)}_N \COMMA
\end{align}
where 
\begin{equation*}
\rho_{d \times N}(\mathbf{z})=\frac{1}{(2 \pi)^{{d \times N}/2}} e^{-\frac{1}{2} \mathbf{z}^T \mathbf{z}} \PERIOD
\end{equation*}

In the discrete case, we can show that the numerical approximation of $X^{(j)}(T)$ satisfies
%\begin{align}
%	\bar{X}^{(j)}_T&=\Phi(\Delta t, Z_1^{(j)}, \Delta \widetilde{B}^{(j)}_0,\dots,\Delta \widetilde{B}^{(j)}_{N-1}),  \quad 1 \le j \le d, \\ \nonumber
%\end{align}
%and precisely, we have
\begin{align}\label{eq:discrete_rep}
	\bar{X}^{(j)}(T)&=X_0^{(j)} \prod_{i=0}^{N-1} \left[ 1+\frac{\sigma^{(j)}}{\sqrt{T}} Z^{(j)}_1 \Delta t+ \sigma^{(j)} \Delta \widetilde{B}^{(j)}_{i}\right], \quad 1 \le j \le d \nonumber\\
	&= \prod_{i=0}^{N-1} f_i^{(j)}(Z^{(j)}_1) , \quad 1 \le j \le d \PERIOD
\end{align}
\subsection{Step $1$: Numerical smoothing}
The first step of our idea is to smoothen the problem by solving the root finding problem in one dimension after using a sub-optimal linear mapping for the coarsest factors of the Brownian increments $\mathbf{Z}_1=(Z^{(1)}_1 , \dots, Z^{(d)}_1)$. In fact, let us define for a certain $d \times d $ matrix $\mathcal{A} $, the linear mapping 
\begin{align}\label{eq:linear_transformation}
\mathbf{Y}&=\mathcal{A} \mathbf{Z}_1 \PERIOD
\end{align}
Then from \eqref{eq:discrete_rep}, we have
 \begin{align}\label{eq:discrete_rep_2}
	\bar{X}^{(j)}(T)&= \prod_{i=0}^{N-1} f_i^{(j)}(\mathcal{A}^{-1} \mathbf{Y})_{j} , \quad 1 \le j \le d \COMMA \nonumber \\
&=\prod_{i=0}^{N-1} g_i^{(j)}(Y_{1},\mathbf{Y}_{-1}) \quad 1 \le j \le d 
\end{align}
where, with defining $\mathcal{A}^{\text{inv}}= \mathcal{A}^{-1}$, we have

\begin{align}\label{eq: incremental functions}
g_i^{(j)}(Y_1,\mathbf{Y}_{-1})&=X_0^{(j)}  \left[ 1+\frac{\sigma^{(j)}}{\sqrt{T}} \left( \sum_{i=1}^d A^{\text{inv}}_{ji} Y_i \right) \Delta t+ \sigma^{(j)} \Delta \widetilde{B}^{(j)}_{i}\right] \nonumber\\
&=X_0^{(j)}  \left[ 1+\frac{\sigma^{(j)} \Delta t}{\sqrt{T}} A^{\text{inv}}_{j1} Y_1 -\frac{\sigma^{(j)}}{\sqrt{T}} \left( \sum_{i=2}^d A^{\text{inv}}_{ji} Y_i  \right) \Delta t+ \sigma^{(j)} \Delta \widetilde{B}^{(j)}_{i}\right]
\end{align}
Therefore, in order to determine $Y^{\ast}_1$, we need to solve
\begin{align}
	x=\sum_{j=1}^{d} c_j \prod_{i=0}^{N-1} g_i^{(j)}(Y^{\ast}_1(x),\mathbf{Y}_{-1} ),
\end{align}
which implies that the location of the kink point for the approximate problem is equivalent to finding the roots of the polynomial $P(Y^\ast_1(K))$, given by
\begin{align}\label{polynomial_kink_location_basket}
	P(Y^\ast_1(K))&=\left(\sum_{j=1}^{d} c_j \prod_{i=0}^{N-1}  g_i^{(j)}(Y^{\ast}_1) \right) -K.
\end{align}
Using  \textbf{Newton iteration method}, we use the expression $P^\prime=\frac{d P}{d Y^\ast_1}$, and we can easily show that
\begin{align}\label{polynomial_kink_location_derivative_basket}
	P^\prime(W_1)=\sum_{j=1}^{d} c_j \frac{\sigma^{(j)} \Delta t A^{\text{inv}}_{j1}} {\sqrt{T}} \left( \prod_{i=0}^{N-1} g_i^{(j)}(Y_1) \right) \left[ \sum_{i=0}^{N-1} \frac{1}{g_i^{(j)}(Y_1)}\right].
\end{align}



\begin{remark}
For our purposes, we suggest already that the coarsest factors of the Brownian increments are the most important ones, compared to the remaining factors. However, one may expect that in case we want to optimize over the choice of the linear mapping $\mathcal{A}$, and which direction is the most important for the kink location, one needs then to solve
\begin{align*}
\underset{\underset{\mathcal{A} \text{ is a rotation}} {\mathcal{A} \in \rset^{d \times d}}}{\operatorname{\sup}} \left(\underset{1 \le i \le d}{\max}\abs{ \frac{\partial g}{\partial Y_i}} \right) \COMMA
\end{align*}
which becomes hard to solve  when $d$ increases.
\end{remark}

\begin{remark}
A general choice for $\mathcal{A}$ should in the family of rotations. However, we think that a sufficiently good matrix $\mathcal{A}$ would be the one leading to $Y_1=\sum_{i=1}^d Z_1^{(i)}$ up to re-scaling.
\end{remark}
\subsection{Step $2$: Integration}


At this stage, we want to perform the pre-integrating step with respect to  $W^\ast_1$. In fact, we have from \eqref{eq: option price as integral_basket}
\begin{small}
\begin{align}\label{eq: pre_integration_step_wrt_y1_basket}
	\expt{g(\mathbf{X}(T))}&=\int_{\rset^{d \times N}} G(z_1^{(1)}, \dots, z_N^{(1)}, \dots, z_1^{(d)},\dots,z^{(d)}_N)) \rho_{d \times N}(\mathbf{z}) dz_1^{(1)} \dots dz_N^{(1)} \dots z_1^{(d)} \dots dz^{(d)}_N \nonumber\\ 
	&=\int_{\rset^{dN-1}} \left(\int_{\rset} G(y_1,\mathbf{y}_{-1},\mathbf{z}^{(1)}_{-1},\dots,\mathbf{z}^{(d)}_{-1} ) \rho_{y_1}(y_1) dy_1 \right)\rho_{d-1}(\mathbf{y}_{-1}) d\mathbf{y}_{-1} \rho_{d \times(N-1)}(\mathbf{z}_{-1}^{(1)},\dots,\mathbf{z}_{-1}^{(d)}) d\mathbf{z}_{-1}^{(1)}\dots d\mathbf{z}_{-1}^{(d)} \nonumber\\	
	&=\int_{\rset^{dN-1}} h(\mathbf{y}_{-1},\mathbf{z}^{(1)}_{-1},\dots,\mathbf{z}^{(d)}_{-1} )\rho_{d-1}(\mathbf{y}_{-1}) d\mathbf{y}_{-1}  \rho_{d\times (N-1)}(\mathbf{z}_{-1}^{(1)},\dots,\mathbf{z}_{-1}^{(d)}) d\mathbf{z}^{(1)}_{-1} \dots d\mathbf{z}^{(d)}_{-1}\COMMA \\ \nonumber
	&=\expt{h(\mathbf{y}_{-1}, \mathbf{z}^{(1)}_{-1},\dots,\mathbf{z}^{(d)}_{-1} )} \COMMA
\end{align}
\end{small}
where
\begin{align}
 h(\mathbf{y}_{-1},\mathbf{z}^{(1)}_{-1},\dots,\mathbf{z}^{(d)}_{-1})&=\int_{\rset} G(y_1,\mathbf{y}_{-1},\mathbf{z}^{(1)}_{-1},\dots,\mathbf{z}^{(d)}_{-1} ) \rho_{y_1}(y_1) dy_1 \nonumber\\
 &= \int_{-\infty}^{y^\ast_1} G(y_1,\mathbf{y}_{-1},\mathbf{z}^{(1)}_{-1},\dots,\mathbf{z}^{(d)}_{-1} ) \rho_{y_1}(y_1) dy_1\nonumber\\
 &+ \int_{y_1^\ast}^{+\infty} G(y_1,\mathbf{y}_{-1},\mathbf{z}^{(1)}_{-1},\dots,\mathbf{z}^{(d)}_{-1} ) \rho_{y_1}(y_1) dW_1
\end{align}

\subsection{The best call option case under GBM and Heston model}

The second example that we consider for multi-dimension is the best call option under GBM and Heston model. I am in process of formulating this but somehow we agreed that the first potential directions of smoothing for the Heston model will be the first factors related to the asset prices.