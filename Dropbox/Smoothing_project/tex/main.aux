\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bungartz2004sparse}
\citation{griebel2013smoothing}
\citation{bayersmoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\citation{xiao2018conditional}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{xiao2018conditional}
\citation{bayersmoothing}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem formulation and Setting}{2}{section.2}}
\newlabel{sec:General setting}{{2}{2}{Problem formulation and Setting}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Continuous time formulation}{2}{subsection.2.1}}
\newlabel{eq:SDE_interest}{{2.1}{2}{Continuous time formulation}{equation.2.1}{}}
\newlabel{eq:smoothing_decomposition}{{2.3}{2}{Continuous time formulation}{equation.2.3}{}}
\newlabel{eq:smoothing_decomposition_componentwise}{{2.4}{2}{Continuous time formulation}{equation.2.4}{}}
\newlabel{eq:SDE_decomposition_componentwise_exapanded}{{2.6}{3}{Continuous time formulation}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}First approach (Brutal)}{3}{subsubsection.2.1.1}}
\newlabel{eq: variance global terms}{{2.7}{3}{First approach (Brutal)}{equation.2.7}{}}
\newlabel{eq:approximate_dynamics}{{2.8}{4}{First approach (Brutal)}{equation.2.8}{}}
\newlabel{eq:first_moment_approximation}{{2.10}{4}{First approach (Brutal)}{equation.2.10}{}}
\newlabel{eq:second_moment_approximation}{{2.11}{4}{First approach (Brutal)}{equation.2.11}{}}
\newlabel{eq:covariance_dynamcis_approximation}{{2.12}{5}{First approach (Brutal)}{equation.2.12}{}}
\newlabel{eq:call_option}{{2.13}{5}{First approach (Brutal)}{equation.2.13}{}}
\newlabel{eq:binary_option}{{2.14}{5}{First approach (Brutal)}{equation.2.14}{}}
\newlabel{assump:Monotonicity condition}{{2.15}{5}{First approach (Brutal)}{equation.2.15}{}}
\newlabel{assump:Growth condition}{{2.16}{5}{First approach (Brutal)}{equation.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Discrete time formulation}{6}{subsection.2.2}}
\newlabel{sec:Discrete time, practical motivation}{{2.2}{6}{Discrete time formulation}{subsection.2.2}{}}
\newlabel{lognormal_dynamics_basket}{{2.18}{6}{Discrete time formulation}{equation.2.18}{}}
\newlabel{eq: option price as integral_basket}{{2.19}{7}{Discrete time formulation}{equation.2.19}{}}
\newlabel{eq:discrete_rep}{{2.20}{7}{Discrete time formulation}{equation.2.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Step $1$: Numerical smoothing}{7}{subsubsection.2.2.1}}
\newlabel{sec:Step $1$: Numerical smoothing}{{2.2.1}{7}{Step $1$: Numerical smoothing}{subsubsection.2.2.1}{}}
\newlabel{eq:linear_transformation}{{2.21}{7}{Step $1$: Numerical smoothing}{equation.2.21}{}}
\newlabel{eq:discrete_rep_2}{{2.22}{7}{Step $1$: Numerical smoothing}{equation.2.22}{}}
\newlabel{eq: incremental functions}{{2.23}{8}{Step $1$: Numerical smoothing}{equation.2.23}{}}
\newlabel{polynomial_kink_location_basket}{{2.25}{8}{Step $1$: Numerical smoothing}{equation.2.25}{}}
\newlabel{polynomial_kink_location_derivative_basket}{{2.26}{8}{Step $1$: Numerical smoothing}{equation.2.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Step $2$: Integration}{8}{subsubsection.2.2.2}}
\newlabel{sec:Step $2$: Integration}{{2.2.2}{8}{Step $2$: Integration}{subsubsection.2.2.2}{}}
\newlabel{eq: pre_integration_step_wrt_y1_basket}{{2.27}{8}{Step $2$: Integration}{equation.2.27}{}}
\newlabel{eq:smooth_function_after_pre_integration}{{2.28}{9}{Step $2$: Integration}{equation.2.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analyticity and Smoothness Analysis}{9}{section.3}}
\newlabel{sec:Analiticity Analysis}{{3}{9}{Analyticity and Smoothness Analysis}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Haar construction of Brownian motion revisited}{9}{subsection.3.1}}
\newlabel{sec:haar-constr-brown}{{3.1}{9}{Haar construction of Brownian motion revisited}{subsection.3.1}{}}
\newlabel{eq:Haar-mother}{{3.1}{9}{Haar construction of Brownian motion revisited}{equation.3.1}{}}
\newlabel{eq:Haar-basis}{{3.2}{9}{Haar construction of Brownian motion revisited}{equation.3.2}{}}
\newlabel{eq:Haar-constant}{{3.2a}{9}{Haar construction of Brownian motion revisited}{equation.3.2a}{}}
\newlabel{eq:Haar-nonconstant}{{3.2b}{9}{Haar construction of Brownian motion revisited}{equation.3.2b}{}}
\newlabel{eq:Haar-int-basis}{{3.3}{9}{Haar construction of Brownian motion revisited}{equation.3.3}{}}
\newlabel{eq:Haar-int-constant}{{3.3a}{9}{Haar construction of Brownian motion revisited}{equation.3.3a}{}}
\newlabel{eq:Haar-int-nonconstant}{{3.3b}{9}{Haar construction of Brownian motion revisited}{equation.3.3b}{}}
\newlabel{eq:Brownian-motion}{{3.4}{9}{Haar construction of Brownian motion revisited}{equation.3.4}{}}
\newlabel{eq:Brownian-motion-truncated}{{3.5}{10}{Haar construction of Brownian motion revisited}{equation.3.5}{}}
\newlabel{eq:increments}{{3.6}{10}{Haar construction of Brownian motion revisited}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Stochastic differential equations}{10}{subsection.3.2}}
\newlabel{sec:stoch-diff-equat}{{3.2}{10}{Stochastic differential equations}{subsection.3.2}{}}
\newlabel{eq:SDE}{{3.7}{10}{Stochastic differential equations}{equation.3.7}{}}
\newlabel{eq:euler}{{3.8}{10}{Stochastic differential equations}{equation.3.8}{}}
\newlabel{eq:H-function}{{3.9}{10}{Stochastic differential equations}{equation.3.9}{}}
\newlabel{eq:1}{{3.10}{11}{Stochastic differential equations}{equation.3.10}{}}
\newlabel{ass:boundedness-derivative}{{3.1}{11}{}{theorem.3.1}{}}
\newlabel{ass:boundedness-inverse}{{3.3}{11}{}{theorem.3.3}{}}
\newlabel{lem:dXdZ}{{3.4}{11}{}{theorem.3.4}{}}
\newlabel{eq:dWdZ}{{3.11}{12}{Stochastic differential equations}{equation.3.11}{}}
\newlabel{eq:2}{{3.12}{12}{Stochastic differential equations}{equation.3.12}{}}
\newlabel{lem:d2XdZdY}{{3.5}{12}{}{theorem.3.5}{}}
\newlabel{eq:d2XdWdW}{{3.13}{12}{Stochastic differential equations}{equation.3.13}{}}
\newlabel{prop:first-derivatives}{{3.7}{12}{}{theorem.3.7}{}}
\newlabel{lem:d2XdZ2}{{3.8}{13}{}{theorem.3.8}{}}
\citation{haji2016multi}
\newlabel{thr:smoothness}{{3.9}{14}{}{theorem.3.9}{}}
\newlabel{rem:analyticity}{{3.10}{14}{}{theorem.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Details of our hierarchical method}{14}{section.4}}
\newlabel{sec:Details of our approach}{{4}{14}{Details of our hierarchical method}{section.4}{}}
\citation{bungartz2004sparse}
\newlabel{eq:total_error}{{4.1}{15}{Details of our hierarchical method}{equation.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The MISC method}{15}{subsection.4.1}}
\newlabel{sec:Details of the MISC}{{4.1}{15}{The MISC method}{subsection.4.1}{}}
\newlabel{eq:MISC_quad_estimator}{{4.2}{16}{The MISC method}{equation.4.2}{}}
\newlabel{eq:quadrature error}{{4.3}{16}{The MISC method}{equation.4.3}{}}
\newlabel{eq:Work_error_contributions}{{4.4}{16}{The MISC method}{equation.4.4}{}}
\citation{morokoff1994quasi}
\citation{moskowitz1996smoothness}
\citation{caflisch1997valuation}
\citation{acworth1998comparison}
\citation{imai2004minimizing}
\citation{glasserman2004monte}
\citation{talay1990expansion}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Brownian bridge (Bb) construction}{17}{subsection.4.2}}
\newlabel{sec:Brwonian bridge construction}{{4.2}{17}{Brownian bridge (Bb) construction}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Richardson extrapolation}{17}{subsection.4.3}}
\newlabel{sec:Richardson extrapolation}{{4.3}{17}{Richardson extrapolation}{subsection.4.3}{}}
\newlabel{Euler_weak_error_strenghten}{{4.5}{17}{Richardson extrapolation}{equation.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Root Finding}{18}{subsection.4.4}}
\newlabel{sec: Root Finding}{{4.4}{18}{Root Finding}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Error discussion}{19}{section.5}}
\newlabel{sec:Error discussion}{{5}{19}{Error discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Errors in smoothing}{19}{subsection.5.1}}
\newlabel{sec:errors-smoothing}{{5.1}{19}{Errors in smoothing}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical experiments}{19}{section.6}}
\newlabel{optimal_number_samples}{{6.1}{20}{Numerical experiments}{Item.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of relative errors and computational gains, achieved by the different methods. In this table, we highlight the computational gains achieved by MISC over MC method to meet a certain error tolerance. We provide details about the way we compute these gains for each case in the following sections.\relax }}{20}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:Summary of our numerical results.}{{6.1}{20}{Summary of relative errors and computational gains, achieved by the different methods. In this table, we highlight the computational gains achieved by MISC over MC method to meet a certain error tolerance. We provide details about the way we compute these gains for each case in the following sections.\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Options under the discretized one dimensional GBM model}{20}{subsection.6.1}}
\newlabel{sec:The discretized 1D Black-Scholes}{{6.1}{20}{Options under the discretized one dimensional GBM model}{subsection.6.1}{}}
\newlabel{lognormal_dynamics}{{6.2}{20}{Options under the discretized one dimensional GBM model}{equation.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Determining the kink location}{21}{subsubsection.6.1.1}}
\newlabel{sec:Determining the kink location}{{6.1.1}{21}{Determining the kink location}{subsubsection.6.1.1}{}}
\newlabel{eq: kink_point_problem}{{6.4}{21}{Exact location of the kink for the continuous problem}{equation.6.4}{}}
\newlabel{xact_location_continuous_problem}{{6.6}{21}{Exact location of the kink for the continuous problem}{equation.6.6}{}}
\newlabel{polynomial_kink_location}{{6.10}{22}{Location of the kink for the discrete problem}{equation.6.10}{}}
\newlabel{polynomial_kink_location_derivative}{{6.11}{22}{Location of the kink for the discrete problem}{equation.6.11}{}}
\newlabel{smoothed_integrand_single_opt_1d}{{6.12}{22}{Location of the kink for the discrete problem}{equation.6.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Results for the single binary option under discretized GBM model}{22}{subsubsection.6.1.2}}
\newlabel{sec:Results for the binary option example}{{6.1.2}{22}{Results for the single binary option under discretized GBM model}{subsubsection.6.1.2}{}}
\newlabel{smoothed_integrand_binary_opt_2}{{6.14}{22}{Results for the single binary option under discretized GBM model}{equation.6.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, using MC with $M=10^4$ samples for the binary option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{23}{figure.caption.4}}
\newlabel{fig:Weak_rate_binary}{{6.1}{23}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, using MC with $M=10^4$ samples for the binary option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Total relative error of MISC, with different tolerances, and MC to compute binary option price for different number of time steps, without Richardson extrapolation. The values marked in red, for MISC method, correspond to the total relative errors associated with stable quadrature errors for MISC, and will be used for complexity comparison against MC.\relax }}{23}{table.caption.5}}
\newlabel{Total error of MISC and MC to compute Binary option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.2}{23}{Total relative error of MISC, with different tolerances, and MC to compute binary option price for different number of time steps, without Richardson extrapolation. The values marked in red, for MISC method, correspond to the total relative errors associated with stable quadrature errors for MISC, and will be used for complexity comparison against MC.\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Comparison of the computational time of MC and MISC, used to compute binary option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{23}{table.caption.6}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute Binary option price for different number of time steps, without Richardson extrapolation}{{6.3}{23}{Comparison of the computational time of MC and MISC, used to compute binary option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Computational work comparison for MISC and MC methods, for the case of binary option. This plot shows that to achieve a relative error below $1\%$, MISC outperforms MC method in terms of computational time\relax }}{24}{figure.caption.7}}
\newlabel{fig:Complexity plot for MC and MISC , Binary, Non rich}{{6.2}{24}{Computational work comparison for MISC and MC methods, for the case of binary option. This plot shows that to achieve a relative error below $1\%$, MISC outperforms MC method in terms of computational time\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Results for the single call option example}{24}{subsubsection.6.1.3}}
\newlabel{sec:Results for the call option example}{{6.1.3}{24}{Results for the single call option example}{subsubsection.6.1.3}{}}
\newlabel{smoothed_integrand_call_opt_2}{{6.16}{24}{Results for the single call option example}{equation.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, using MC with $M=4 \times 10^5$ samples for the call option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{25}{figure.caption.8}}
\newlabel{fig:Weak_rate_call_beta_32}{{6.3}{25}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, using MC with $M=4 \times 10^5$ samples for the call option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Total relative error of MISC, with different tolerances, and MC to compute call option price for different number of time steps, without Richardson extrapolation. The values marked in red, for MISC method, correspond to the total relative errors associated with stable quadrature errors for MISC, and will be used for complexity comparison against MC.\relax }}{25}{table.caption.9}}
\newlabel{Total error of MISC and MC to compute Call option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.4}{25}{Total relative error of MISC, with different tolerances, and MC to compute call option price for different number of time steps, without Richardson extrapolation. The values marked in red, for MISC method, correspond to the total relative errors associated with stable quadrature errors for MISC, and will be used for complexity comparison against MC.\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Comparison of the computational time of MC and MISC, used to compute call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{25}{table.caption.10}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute Call option price for different number of time steps, without Richardson extrapolation}{{6.5}{25}{Comparison of the computational time of MC and MISC, used to compute call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Computational work comparison for MISC and MC methods, for the case of call option. This plot shows that to achieve a relative error below $1\%$, MISC outperforms significantly MC method in terms of computational time.\relax }}{26}{figure.caption.11}}
\newlabel{fig:Complexity plot for MC and MISC , Call non rich}{{6.4}{26}{Computational work comparison for MISC and MC methods, for the case of call option. This plot shows that to achieve a relative error below $1\%$, MISC outperforms significantly MC method in terms of computational time.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}The basket call option under GBM model}{26}{subsection.6.2}}
\newlabel{sec:The basket call option under GBM model}{{6.2}{26}{The basket call option under GBM model}{subsection.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}$d=2$}{26}{subsection.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, for the two dimensional basket call option with a number of Laguerre quadrature points $\beta =128$ and number of samples for MC $M=10^7$. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{26}{figure.caption.12}}
\newlabel{fig:Weak_rate_two_dim_basket}{{6.5}{26}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, for the two dimensional basket call option with a number of Laguerre quadrature points $\beta =128$ and number of samples for MC $M=10^7$. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Total relative error of MISC, with different tolerances, and MC to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values marked in red, for MISC method, correspond to the total relative errors associated with stable quadrature errors for MISC, and will be used for complexity comparison against MC.\relax }}{27}{table.caption.13}}
\newlabel{Total error of MISC and MC to compute two dim basket Call option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.6}{27}{Total relative error of MISC, with different tolerances, and MC to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values marked in red, for MISC method, correspond to the total relative errors associated with stable quadrature errors for MISC, and will be used for complexity comparison against MC.\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces Comparison of the computational time of MC and MISC, used to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{27}{table.caption.14}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute two dim basket Call option price for different number of time steps, without Richardson extrapolation}{{6.7}{27}{Comparison of the computational time of MC and MISC, used to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Computational work comparison for MISC and MC methods, for the case of two dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, MISC outperforms significantly MC method in terms of computational time.\relax }}{27}{figure.caption.15}}
\newlabel{fig:Complexity plot for MC and MISC , two dim basket call non rich}{{6.6}{27}{Computational work comparison for MISC and MC methods, for the case of two dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, MISC outperforms significantly MC method in terms of computational time.\relax }{figure.caption.15}{}}
\bibstyle{plain}
\bibdata{smoothing}
\bibcite{acworth1998comparison}{1}
\bibcite{bayersmoothing}{2}
\bibcite{bungartz2004sparse}{3}
\bibcite{caflisch1997valuation}{4}
\bibcite{glasserman2004monte}{5}
\bibcite{griebel2013smoothing}{6}
\bibcite{griebel2017note}{7}
\bibcite{griewank2017high}{8}
\bibcite{haji2016multi}{9}
\bibcite{imai2004minimizing}{10}
\bibcite{morokoff1994quasi}{11}
\bibcite{moskowitz1996smoothness}{12}
\bibcite{talay1990expansion}{13}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}$d=4$}{28}{subsection.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}The best call option under GBM and Heston model}{28}{subsection.6.5}}
\newlabel{sec:The best call option under GBM and Heston model}{{6.5}{28}{The best call option under GBM and Heston model}{subsection.6.5}{}}
\bibcite{xiao2018conditional}{14}
