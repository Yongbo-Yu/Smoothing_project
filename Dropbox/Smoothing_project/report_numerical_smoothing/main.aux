\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bungartz2004sparse}
\citation{griebel2013smoothing}
\citation{bayersmoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\citation{xiao2018conditional}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{xiao2018conditional}
\citation{bayersmoothing}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem formulation and Setting}{2}{section.2}}
\newlabel{sec:General setting}{{2}{2}{Problem formulation and Setting}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Continuous time formulation}{2}{subsection.2.1}}
\newlabel{eq:SDE_interest}{{2.1}{2}{Continuous time formulation}{equation.2.1}{}}
\newlabel{eq:smoothing_decomposition}{{2.3}{2}{Continuous time formulation}{equation.2.3}{}}
\newlabel{eq:smoothing_decomposition_componentwise}{{2.4}{2}{Continuous time formulation}{equation.2.4}{}}
\newlabel{eq:SDE_decomposition_componentwise_exapanded}{{2.6}{3}{Continuous time formulation}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}First approach (Brutal)}{3}{subsubsection.2.1.1}}
\newlabel{eq: variance global terms}{{2.7}{3}{First approach (Brutal)}{equation.2.7}{}}
\newlabel{eq:approximate_dynamics}{{2.8}{4}{First approach (Brutal)}{equation.2.8}{}}
\newlabel{eq:first_moment_approximation}{{2.10}{4}{First approach (Brutal)}{equation.2.10}{}}
\newlabel{eq:second_moment_approximation}{{2.11}{4}{First approach (Brutal)}{equation.2.11}{}}
\newlabel{eq:covariance_dynamcis_approximation}{{2.12}{5}{First approach (Brutal)}{equation.2.12}{}}
\newlabel{eq:call_option}{{2.13}{5}{First approach (Brutal)}{equation.2.13}{}}
\newlabel{eq:binary_option}{{2.14}{5}{First approach (Brutal)}{equation.2.14}{}}
\newlabel{assump:Monotonicity condition}{{2.15}{6}{First approach (Brutal)}{equation.2.15}{}}
\newlabel{assump:Growth condition}{{2.16}{6}{First approach (Brutal)}{equation.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Discrete time formulation}{6}{subsection.2.2}}
\newlabel{sec:Discrete time, practical motivation}{{2.2}{6}{Discrete time formulation}{subsection.2.2}{}}
\newlabel{lognormal_dynamics_basket}{{2.18}{6}{Discrete time formulation}{equation.2.18}{}}
\newlabel{eq: option price as integral_basket}{{2.19}{6}{Discrete time formulation}{equation.2.19}{}}
\newlabel{eq:discrete_rep}{{2.20}{7}{Discrete time formulation}{equation.2.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Step $1$: Numerical smoothing}{7}{subsubsection.2.2.1}}
\newlabel{sec:Step $1$: Numerical smoothing}{{2.2.1}{7}{Step $1$: Numerical smoothing}{subsubsection.2.2.1}{}}
\newlabel{eq:linear_transformation}{{2.21}{7}{Step $1$: Numerical smoothing}{equation.2.21}{}}
\newlabel{eq:discrete_rep_2}{{2.22}{7}{Step $1$: Numerical smoothing}{equation.2.22}{}}
\newlabel{eq: incremental functions}{{2.23}{7}{Step $1$: Numerical smoothing}{equation.2.23}{}}
\newlabel{polynomial_kink_location_basket}{{2.25}{7}{Step $1$: Numerical smoothing}{equation.2.25}{}}
\newlabel{polynomial_kink_location_derivative_basket}{{2.26}{7}{Step $1$: Numerical smoothing}{equation.2.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Step $2$: Integration}{8}{subsubsection.2.2.2}}
\newlabel{sec:Step $2$: Integration}{{2.2.2}{8}{Step $2$: Integration}{subsubsection.2.2.2}{}}
\newlabel{eq: pre_integration_step_wrt_y1_basket}{{2.27}{8}{Step $2$: Integration}{equation.2.27}{}}
\newlabel{eq:smooth_function_after_pre_integration}{{2.28}{8}{Step $2$: Integration}{equation.2.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analyticity and Smoothness Analysis}{8}{section.3}}
\newlabel{sec:Analiticity Analysis}{{3}{8}{Analyticity and Smoothness Analysis}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Haar construction of Brownian motion revisited}{8}{subsection.3.1}}
\newlabel{sec:haar-constr-brown}{{3.1}{8}{Haar construction of Brownian motion revisited}{subsection.3.1}{}}
\newlabel{eq:Haar-mother}{{3.1}{9}{Haar construction of Brownian motion revisited}{equation.3.1}{}}
\newlabel{eq:Haar-basis}{{3.2}{9}{Haar construction of Brownian motion revisited}{equation.3.2}{}}
\newlabel{eq:Haar-constant}{{3.2a}{9}{Haar construction of Brownian motion revisited}{equation.3.2a}{}}
\newlabel{eq:Haar-nonconstant}{{3.2b}{9}{Haar construction of Brownian motion revisited}{equation.3.2b}{}}
\newlabel{eq:Haar-int-basis}{{3.3}{9}{Haar construction of Brownian motion revisited}{equation.3.3}{}}
\newlabel{eq:Haar-int-constant}{{3.3a}{9}{Haar construction of Brownian motion revisited}{equation.3.3a}{}}
\newlabel{eq:Haar-int-nonconstant}{{3.3b}{9}{Haar construction of Brownian motion revisited}{equation.3.3b}{}}
\newlabel{eq:Brownian-motion}{{3.4}{9}{Haar construction of Brownian motion revisited}{equation.3.4}{}}
\newlabel{eq:Brownian-motion-truncated}{{3.5}{9}{Haar construction of Brownian motion revisited}{equation.3.5}{}}
\newlabel{eq:increments}{{3.6}{9}{Haar construction of Brownian motion revisited}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Stochastic differential equations}{9}{subsection.3.2}}
\newlabel{sec:stoch-diff-equat}{{3.2}{9}{Stochastic differential equations}{subsection.3.2}{}}
\newlabel{eq:SDE}{{3.7}{9}{Stochastic differential equations}{equation.3.7}{}}
\newlabel{eq:euler}{{3.8}{10}{Stochastic differential equations}{equation.3.8}{}}
\newlabel{eq:H-function}{{3.9}{10}{Stochastic differential equations}{equation.3.9}{}}
\newlabel{eq:1}{{3.10}{10}{Stochastic differential equations}{equation.3.10}{}}
\newlabel{ass:boundedness-derivative}{{3.1}{11}{}{theorem.3.1}{}}
\newlabel{ass:boundedness-inverse}{{3.3}{11}{}{theorem.3.3}{}}
\newlabel{lem:dXdZ}{{3.4}{11}{}{theorem.3.4}{}}
\newlabel{eq:dWdZ}{{3.11}{11}{Stochastic differential equations}{equation.3.11}{}}
\newlabel{eq:2}{{3.12}{11}{Stochastic differential equations}{equation.3.12}{}}
\newlabel{lem:d2XdZdY}{{3.5}{12}{}{theorem.3.5}{}}
\newlabel{eq:d2XdWdW}{{3.13}{12}{Stochastic differential equations}{equation.3.13}{}}
\newlabel{prop:first-derivatives}{{3.7}{12}{}{theorem.3.7}{}}
\newlabel{lem:d2XdZ2}{{3.8}{13}{}{theorem.3.8}{}}
\newlabel{thr:smoothness}{{3.9}{13}{}{theorem.3.9}{}}
\newlabel{rem:analyticity}{{3.10}{13}{}{theorem.3.10}{}}
\citation{haji2016multi}
\@writefile{toc}{\contentsline {section}{\numberline {4}Details of our hierarchical method}{14}{section.4}}
\newlabel{sec:Details of our approach}{{4}{14}{Details of our hierarchical method}{section.4}{}}
\newlabel{eq:total_error}{{4.1}{14}{Details of our hierarchical method}{equation.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Adaptive Sparse Grids}{14}{subsection.4.1}}
\newlabel{sec:Details of the ASGQ}{{4.1}{14}{Adaptive Sparse Grids}{subsection.4.1}{}}
\citation{bungartz2004sparse}
\citation{morokoff1994quasi}
\citation{moskowitz1996smoothness}
\citation{caflisch1997valuation}
\citation{acworth1998comparison}
\citation{imai2004minimizing}
\citation{glasserman2004monte}
\newlabel{eq:MISC_quad_estimator}{{4.2}{16}{Adaptive Sparse Grids}{equation.4.2}{}}
\newlabel{eq:quadrature error}{{4.3}{16}{Adaptive Sparse Grids}{equation.4.3}{}}
\newlabel{eq:Work_error_contributions}{{4.4}{16}{Adaptive Sparse Grids}{equation.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Brownian bridge (Bb) construction}{16}{subsection.4.2}}
\newlabel{sec:Brwonian bridge construction}{{4.2}{16}{Brownian bridge (Bb) construction}{subsection.4.2}{}}
\citation{talay1990expansion}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Richardson extrapolation}{17}{subsection.4.3}}
\newlabel{sec:Richardson extrapolation}{{4.3}{17}{Richardson extrapolation}{subsection.4.3}{}}
\newlabel{Euler_weak_error_strenghten}{{4.5}{17}{Richardson extrapolation}{equation.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Root Finding}{17}{subsection.4.4}}
\newlabel{sec: Root Finding}{{4.4}{17}{Root Finding}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Error discussion}{18}{section.5}}
\newlabel{sec:Error discussion}{{5}{18}{Error discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Errors in smoothing}{18}{subsection.5.1}}
\newlabel{sec:errors-smoothing}{{5.1}{18}{Errors in smoothing}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical experiments for Numerical smoothing with ASGQ}{19}{section.6}}
\newlabel{optimal_number_samples}{{6.1}{19}{Numerical experiments for Numerical smoothing with ASGQ}{Item.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of relative errors and computational gains, achieved by the different methods. In this table, we highlight the computational gains achieved by ASGQ over MC method to meet a certain error tolerance. We provide details about the way we compute these gains for each case in the following sections.\relax }}{20}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:Summary of our numerical results.}{{6.1}{20}{Summary of relative errors and computational gains, achieved by the different methods. In this table, we highlight the computational gains achieved by ASGQ over MC method to meet a certain error tolerance. We provide details about the way we compute these gains for each case in the following sections.\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Options under the discretized one dimensional GBM model}{20}{subsection.6.1}}
\newlabel{sec:The discretized 1D Black-Scholes}{{6.1}{20}{Options under the discretized one dimensional GBM model}{subsection.6.1}{}}
\newlabel{lognormal_dynamics}{{6.2}{20}{Options under the discretized one dimensional GBM model}{equation.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Determining the kink location}{21}{subsubsection.6.1.1}}
\newlabel{sec:Determining the kink location}{{6.1.1}{21}{Determining the kink location}{subsubsection.6.1.1}{}}
\newlabel{eq: kink_point_problem}{{6.4}{21}{Exact location of the kink for the continuous problem}{equation.6.4}{}}
\newlabel{xact_location_continuous_problem}{{6.6}{21}{Exact location of the kink for the continuous problem}{equation.6.6}{}}
\newlabel{polynomial_kink_location}{{6.10}{21}{Location of the kink for the discrete problem}{equation.6.10}{}}
\newlabel{polynomial_kink_location_derivative}{{6.11}{22}{Location of the kink for the discrete problem}{equation.6.11}{}}
\newlabel{smoothed_integrand_single_opt_1d}{{6.12}{22}{Location of the kink for the discrete problem}{equation.6.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Results for the single binary option under discretized GBM model}{22}{subsubsection.6.1.2}}
\newlabel{sec:Results for the binary option example}{{6.1.2}{22}{Results for the single binary option under discretized GBM model}{subsubsection.6.1.2}{}}
\newlabel{smoothed_integrand_binary_opt_2}{{6.14}{22}{Results for the single binary option under discretized GBM model}{equation.6.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, using MC with $M=10^4$ samples for the binary option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{23}{figure.caption.4}}
\newlabel{fig:Weak_rate_binary}{{6.1}{23}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, using MC with $M=10^4$ samples for the binary option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute binary option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{23}{table.caption.5}}
\newlabel{Total error of MISC and MC to compute Binary option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.2}{23}{Total relative error of ASGQ, with different tolerances, and MC to compute binary option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute binary option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{23}{table.caption.6}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute Binary option price for different number of time steps, without Richardson extrapolation}{{6.3}{23}{Comparison of the computational time of MC and ASGQ, used to compute binary option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of binary option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms MC method in terms of computational time\relax }}{24}{figure.caption.7}}
\newlabel{fig:Complexity plot for MC and MISC , Binary, Non rich}{{6.2}{24}{Computational work comparison for ASGQ and MC methods, for the case of binary option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms MC method in terms of computational time\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Results for the single call option example}{24}{subsubsection.6.1.3}}
\newlabel{sec:Results for the call option example}{{6.1.3}{24}{Results for the single call option example}{subsubsection.6.1.3}{}}
\newlabel{smoothed_integrand_call_opt_2}{{6.16}{24}{Results for the single call option example}{equation.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, using MC with $M=4 \times 10^5$ samples for the call option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{25}{figure.caption.8}}
\newlabel{fig:Weak_rate_call_beta_32}{{6.3}{25}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, using MC with $M=4 \times 10^5$ samples for the call option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{25}{table.caption.9}}
\newlabel{Total error of MISC and MC to compute Call option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.4}{25}{Total relative error of ASGQ, with different tolerances, and MC to compute call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{25}{table.caption.10}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute Call option price for different number of time steps, without Richardson extrapolation}{{6.5}{25}{Comparison of the computational time of MC and ASGQ, used to compute call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }}{26}{figure.caption.11}}
\newlabel{fig:Complexity plot for MC and MISC , Call non rich}{{6.4}{26}{Computational work comparison for ASGQ and MC methods, for the case of call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}The basket call option under GBM model}{26}{subsection.6.2}}
\newlabel{sec:The basket call option under GBM model}{{6.2}{26}{The basket call option under GBM model}{subsection.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}$d=2$}{26}{subsection.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, for the two dimensional basket call option with a number of Laguerre quadrature points $\beta =128$ and number of samples for MC $M=10^7$. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{26}{figure.caption.12}}
\newlabel{fig:Weak_rate_two_dim_basket}{{6.5}{26}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, for the two dimensional basket call option with a number of Laguerre quadrature points $\beta =128$ and number of samples for MC $M=10^7$. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{27}{table.caption.13}}
\newlabel{Total error of MISC and MC to compute two dim basket Call option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.6}{27}{Total relative error of ASGQ, with different tolerances, and MC to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{27}{table.caption.14}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute two dim basket Call option price for different number of time steps, without Richardson extrapolation}{{6.7}{27}{Comparison of the computational time of MC and ASGQ, used to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of two dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }}{27}{figure.caption.15}}
\newlabel{fig:Complexity plot for MC and MISC , two dim basket call non rich}{{6.6}{27}{Computational work comparison for ASGQ and MC methods, for the case of two dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}$d=4$}{28}{subsection.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, for the $4$-dimensional basket call option with a number of Laguerre quadrature points $\beta =512$ and number of samples for MC $M=4 \times 10^6$. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{28}{figure.caption.16}}
\newlabel{fig:Weak_rate_4_dim_basket}{{6.7}{28}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, for the $4$-dimensional basket call option with a number of Laguerre quadrature points $\beta =512$ and number of samples for MC $M=4 \times 10^6$. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.8}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute $4$-dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{28}{table.caption.17}}
\newlabel{Total error of MISC and MC to compute 4 dim basket Call option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.8}{28}{Total relative error of ASGQ, with different tolerances, and MC to compute $4$-dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.17}{}}
\citation{heston1993closed}
\citation{broadie2006exact}
\citation{kahl2006fast}
\citation{andersen2007efficient}
\@writefile{lot}{\contentsline {table}{\numberline {6.9}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute $4$-dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{29}{table.caption.18}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute 4 dim basket Call option price for different number of time steps, without Richardson extrapolation}{{6.9}{29}{Comparison of the computational time of MC and ASGQ, used to compute $4$-dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of $4$-dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }}{29}{figure.caption.19}}
\newlabel{fig:Complexity plot for MC and MISC , 4 dim basket call non rich}{{6.8}{29}{Computational work comparison for ASGQ and MC methods, for the case of $4$-dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Options under the discretized the Heston model}{29}{section.7}}
\newlabel{eq:dynamics Heston}{{7.1}{29}{Options under the discretized the Heston model}{equation.7.1}{}}
\citation{lord2010comparison}
\citation{lord2010comparison}
\citation{andersen2005extended}
\citation{andersen2007efficient}
\citation{andersen2005extended}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Fixed Euler scheme}{30}{subsection.7.1}}
\newlabel{sec:Discretization of Heston model with a non smooth transformations for the volatility process}{{7.1}{30}{Fixed Euler scheme}{subsection.7.1}{}}
\newlabel{eq:FE_Heston_discrete}{{7.2}{30}{Fixed Euler scheme}{equation.7.2}{}}
\newlabel{Numerical schemes for CIR process}{{\caption@xref {Numerical schemes for CIR process}{ on input line 1251}}{30}{Fixed Euler scheme}{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Euler schemes with moment matching}{30}{subsection.7.2}}
\newlabel{sec:Euler schemes with moment matching}{{7.2}{30}{Euler schemes with moment matching}{subsection.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}The ABR method}{30}{subsubsection.7.2.1}}
\newlabel{sec:The ABR method}{{7.2.1}{30}{The ABR method}{subsubsection.7.2.1}{}}
\citation{lord2010comparison}
\citation{andersen2007efficient}
\citation{andersen2005extended}
\citation{andersen2007efficient}
\newlabel{eq: vol_moment_matching}{{7.3}{31}{The ABR method}{equation.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}The QE method}{31}{subsubsection.7.2.2}}
\newlabel{sec:The QE method}{{7.2.2}{31}{The QE method}{subsubsection.7.2.2}{}}
\citation{kahl2006fast}
\citation{kahl2006fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Kahl-Jackel Scheme}{32}{subsection.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{32}{subsection.7.4}}
\newlabel{sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{{7.4}{32}{Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{subsection.7.4}{}}
\newlabel{equivalent OU process}{{7.5}{33}{Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{equation.7.5}{}}
\newlabel{eq:expressing CIR processes from OU processes}{{7.8}{33}{Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{equation.7.8}{}}
\citation{jeanblanc2009mathematical}
\citation{andersen2007efficient}
\citation{lord2010comparison}
\citation{alfonsi2010high}
\citation{haji2016multi}
\citation{lord2010comparison}
\citation{broadie2006exact}
\citation{heston1993closed}
\citation{heston1993closed}
\newlabel{eq:equivalent CIR process}{{7.12}{34}{Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{equation.7.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}On the choice of the simulation scheme of the Heston model}{34}{subsection.7.5}}
\newlabel{sec:On the choice of the simulation scheme of the Heston model}{{7.5}{34}{On the choice of the simulation scheme of the Heston model}{subsection.7.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}Comparison in terms of mixed differences rates}{34}{subsubsection.7.5.1}}
\newlabel{sec:Comparison in terms of mixed differences rates}{{7.5.1}{34}{Comparison in terms of mixed differences rates}{subsubsection.7.5.1}{}}
\newlabel{eq:Work_error_contributions}{{7.13}{34}{Comparison in terms of mixed differences rates}{equation.7.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Reference solution using Premia with cf\_call\_heston method as explained in \cite  {heston1993closed}, for different parameter constellations. By $n$ we refer to the number of OU processes for simulating the volatility process in approach given by Section \ref  {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}\relax }}{35}{table.caption.21}}
\newlabel{table:Reference solution, for different parameter constellations.}{{7.1}{35}{Reference solution using Premia with cf\_call\_heston method as explained in \cite {heston1993closed}, for different parameter constellations. By $n$ we refer to the number of OU processes for simulating the volatility process in approach given by Section \ref {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}\relax }{table.caption.21}{}}
\newlabel{fig:1}{{7.1a}{36}{\relax }{figure.caption.22}{}}
\newlabel{sub@fig:1}{{a}{36}{\relax }{figure.caption.22}{}}
\newlabel{fig:2}{{7.1b}{36}{\relax }{figure.caption.22}{}}
\newlabel{sub@fig:2}{{b}{36}{\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces $100$ paths of process $X$ given by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {equivalent OU process}\unskip \@@italiccorr )}} simulated using forward Euler. (a) Case of set $1$ parameters in Table \ref  {table:Reference solution, for different parameter constellations.}, (b) Case of set $3$ parameters in Table \ref  {table:Reference solution, for different parameter constellations.}.\relax }}{36}{figure.caption.22}}
\newlabel{fig:paths of process $X$ simulated using forward Euler}{{7.1}{36}{$100$ paths of process $X$ given by \eqref {equivalent OU process} simulated using forward Euler. (a) Case of set $1$ parameters in Table \ref {table:Reference solution, for different parameter constellations.}, (b) Case of set $3$ parameters in Table \ref {table:Reference solution, for different parameter constellations.}.\relax }{figure.caption.22}{}}
\newlabel{fig:1}{{7.2a}{37}{\relax }{figure.caption.24}{}}
\newlabel{sub@fig:1}{{a}{37}{\relax }{figure.caption.24}{}}
\newlabel{fig:2}{{7.2b}{37}{\relax }{figure.caption.24}{}}
\newlabel{sub@fig:2}{{b}{37}{\relax }{figure.caption.24}{}}
\newlabel{fig:3}{{7.2c}{37}{\relax }{figure.caption.24}{}}
\newlabel{sub@fig:3}{{c}{37}{\relax }{figure.caption.24}{}}
\newlabel{fig:4}{{7.2d}{37}{\relax }{figure.caption.24}{}}
\newlabel{sub@fig:4}{{d}{37}{\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The rate of error convergence of first order differences $\left \delimiter 69640972 \Delta \text  {E}_{\boldsymbol  {\beta }}\right \delimiter 86418188 $, defined by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:Work_error_contributions}\unskip \@@italiccorr )}}, ($\boldsymbol  {\beta }=\mathbf  {1}+k \overline  {\boldsymbol  {\beta }}$) for the example of single call option under Heston model, with parameters given by Set $1$ in Table \ref  {table:Reference solution, for different parameter constellations.}, using $N=4$ time steps. In this case, we just show the first $4$ dimensions which are used for the volatility noise (mainly $dW_v$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:dynamics Heston}\unskip \@@italiccorr )}}). (a) using full truncation as in Section \ref  {sec:Discretization of Heston model with a non smooth transformations for the volatility process}, (b) using the ABR scheme as in Section \ref  {sec:The ABR method}, (c) using the QE scheme as in Section \ref  {sec:The QE method}, (d) using the smooth transformation as in Section \ref  {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}.\relax }}{37}{figure.caption.24}}
\newlabel{fig:first_diff_Heston_call_N_4_set2}{{7.2}{37}{The rate of error convergence of first order differences $\abs {\Delta \text {E}_{\boldsymbol {\beta }}}$, defined by \eqref {eq:Work_error_contributions}, ($\boldsymbol {\beta }=\mathbf {1}+k \bar {\boldsymbol {\beta }}$) for the example of single call option under Heston model, with parameters given by Set $1$ in Table \ref {table:Reference solution, for different parameter constellations.}, using $N=4$ time steps. In this case, we just show the first $4$ dimensions which are used for the volatility noise (mainly $dW_v$ in \eqref {eq:dynamics Heston}). (a) using full truncation as in Section \ref {sec:Discretization of Heston model with a non smooth transformations for the volatility process}, (b) using the ABR scheme as in Section \ref {sec:The ABR method}, (c) using the QE scheme as in Section \ref {sec:The QE method}, (d) using the smooth transformation as in Section \ref {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}.\relax }{figure.caption.24}{}}
\newlabel{fig:1}{{7.3a}{38}{\relax }{figure.caption.26}{}}
\newlabel{sub@fig:1}{{a}{38}{\relax }{figure.caption.26}{}}
\newlabel{fig:2}{{7.3b}{38}{\relax }{figure.caption.26}{}}
\newlabel{sub@fig:2}{{b}{38}{\relax }{figure.caption.26}{}}
\newlabel{fig:3}{{7.3c}{38}{\relax }{figure.caption.26}{}}
\newlabel{sub@fig:3}{{c}{38}{\relax }{figure.caption.26}{}}
\newlabel{fig:4}{{7.3d}{38}{\relax }{figure.caption.26}{}}
\newlabel{sub@fig:4}{{d}{38}{\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The rate of error convergence of first order differences $\left \delimiter 69640972 \Delta \text  {E}_{\boldsymbol  {\beta }}\right \delimiter 86418188 $, defined by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:Work_error_contributions}\unskip \@@italiccorr )}}, ($\boldsymbol  {\beta }=\mathbf  {1}+k \overline  {\boldsymbol  {\beta }}$) for the example of single call option under Heston model, with parameters given by Set $2$ in Table \ref  {table:Reference solution, for different parameter constellations.}, using $N=4$ time steps. In this case, we just show the first $4$ dimensions which are used for the volatility noise (mainly $dW_v$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:dynamics Heston}\unskip \@@italiccorr )}}). (a) using full truncation as in Section \ref  {sec:Discretization of Heston model with a non smooth transformations for the volatility process}, (b) using the ABR scheme as in Section \ref  {sec:The ABR method}, (c) using the QE scheme as in Section \ref  {sec:The QE method}, (d) using the smooth transformation as in Section \ref  {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}, here $N=2$.\relax }}{38}{figure.caption.26}}
\newlabel{fig:first_diff_Heston_call_N_4_set3}{{7.3}{38}{The rate of error convergence of first order differences $\abs {\Delta \text {E}_{\boldsymbol {\beta }}}$, defined by \eqref {eq:Work_error_contributions}, ($\boldsymbol {\beta }=\mathbf {1}+k \bar {\boldsymbol {\beta }}$) for the example of single call option under Heston model, with parameters given by Set $2$ in Table \ref {table:Reference solution, for different parameter constellations.}, using $N=4$ time steps. In this case, we just show the first $4$ dimensions which are used for the volatility noise (mainly $dW_v$ in \eqref {eq:dynamics Heston}). (a) using full truncation as in Section \ref {sec:Discretization of Heston model with a non smooth transformations for the volatility process}, (b) using the ABR scheme as in Section \ref {sec:The ABR method}, (c) using the QE scheme as in Section \ref {sec:The QE method}, (d) using the smooth transformation as in Section \ref {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}, here $N=2$.\relax }{figure.caption.26}{}}
\newlabel{fig:1}{{7.4a}{39}{\relax }{figure.caption.28}{}}
\newlabel{sub@fig:1}{{a}{39}{\relax }{figure.caption.28}{}}
\newlabel{fig:2}{{7.4b}{39}{\relax }{figure.caption.28}{}}
\newlabel{sub@fig:2}{{b}{39}{\relax }{figure.caption.28}{}}
\newlabel{fig:3}{{7.4c}{39}{\relax }{figure.caption.28}{}}
\newlabel{sub@fig:3}{{c}{39}{\relax }{figure.caption.28}{}}
\newlabel{fig:4}{{7.4d}{39}{\relax }{figure.caption.28}{}}
\newlabel{sub@fig:4}{{d}{39}{\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces The rate of error convergence of first order differences $\left \delimiter 69640972 \Delta \text  {E}_{\boldsymbol  {\beta }}\right \delimiter 86418188 $, defined by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:Work_error_contributions}\unskip \@@italiccorr )}}, ($\boldsymbol  {\beta }=\mathbf  {1}+k \overline  {\boldsymbol  {\beta }}$) for the example of single call option under Heston model, with parameters given by Set $3$ in Table \ref  {table:Reference solution, for different parameter constellations.}, using $N=4$ time steps. In this case, we just show the first $4$ dimensions which are used for the volatility noise (mainly $dW_v$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:dynamics Heston}\unskip \@@italiccorr )}}). (a) using full truncation as in Section \ref  {sec:Discretization of Heston model with a non smooth transformations for the volatility process}, (b) using the ABR scheme as in Section \ref  {sec:The ABR method}, (c) using the QE scheme as in Section \ref  {sec:The QE method}, (d) using the smooth transformation as in Section \ref  {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}.\relax }}{39}{figure.caption.28}}
\newlabel{fig:first_diff_Heston_call_N_4_set4}{{7.4}{39}{The rate of error convergence of first order differences $\abs {\Delta \text {E}_{\boldsymbol {\beta }}}$, defined by \eqref {eq:Work_error_contributions}, ($\boldsymbol {\beta }=\mathbf {1}+k \bar {\boldsymbol {\beta }}$) for the example of single call option under Heston model, with parameters given by Set $3$ in Table \ref {table:Reference solution, for different parameter constellations.}, using $N=4$ time steps. In this case, we just show the first $4$ dimensions which are used for the volatility noise (mainly $dW_v$ in \eqref {eq:dynamics Heston}). (a) using full truncation as in Section \ref {sec:Discretization of Heston model with a non smooth transformations for the volatility process}, (b) using the ABR scheme as in Section \ref {sec:The ABR method}, (c) using the QE scheme as in Section \ref {sec:The QE method}, (d) using the smooth transformation as in Section \ref {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}Comparison in terms of the weak error behavior}{39}{subsubsection.7.5.2}}
\newlabel{sec:Comparison in terms of the weak error behavior}{{7.5.2}{39}{Comparison in terms of the weak error behavior}{subsubsection.7.5.2}{}}
\citation{lord2010comparison}
\newlabel{fig:1_weak_error_smooth_vol_set1}{{7.5a}{40}{\relax }{figure.caption.30}{}}
\newlabel{sub@fig:1_weak_error_smooth_vol_set1}{{a}{40}{\relax }{figure.caption.30}{}}
\newlabel{fig:2}{{7.5b}{40}{\relax }{figure.caption.30}{}}
\newlabel{sub@fig:2}{{b}{40}{\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, for the European call option under the discretized Heston model, for Set $1$ parameters in Table \ref  {table:Reference solution, for different parameter constellations.}. The upper and lower bounds are $95\%$ confidence intervals. (a) using the smooth transformation as in Section \ref  {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}, (b) using the ABR scheme as in Section \ref  {sec:The ABR method}.\relax }}{40}{figure.caption.30}}
\newlabel{fig:weak convergence comparison set 1}{{7.5}{40}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, for the European call option under the discretized Heston model, for Set $1$ parameters in Table \ref {table:Reference solution, for different parameter constellations.}. The upper and lower bounds are $95\%$ confidence intervals. (a) using the smooth transformation as in Section \ref {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}, (b) using the ABR scheme as in Section \ref {sec:The ABR method}.\relax }{figure.caption.30}{}}
\newlabel{fig:1}{{7.6a}{41}{\relax }{figure.caption.32}{}}
\newlabel{sub@fig:1}{{a}{41}{\relax }{figure.caption.32}{}}
\newlabel{fig:2}{{7.6b}{41}{\relax }{figure.caption.32}{}}
\newlabel{sub@fig:2}{{b}{41}{\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, for the European call option under the discretized Heston model, for Set $2$ parameters in Table \ref  {table:Reference solution, for different parameter constellations.}. The upper and lower bounds are $95\%$ confidence intervals. (a) using the smooth transformation as in Section \ref  {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}, (b) using the ABR scheme as in Section \ref  {sec:The ABR method}.\relax }}{41}{figure.caption.32}}
\newlabel{fig:weak convergence comparison set 2}{{7.6}{41}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, for the European call option under the discretized Heston model, for Set $2$ parameters in Table \ref {table:Reference solution, for different parameter constellations.}. The upper and lower bounds are $95\%$ confidence intervals. (a) using the smooth transformation as in Section \ref {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}, (b) using the ABR scheme as in Section \ref {sec:The ABR method}.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error},using the ABR scheme as in Section \ref  {sec:The ABR method}, for the European call option under the discretized Heston model, for Set $3$ parameters in Table \ref  {table:Reference solution, for different parameter constellations.}. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{41}{figure.caption.34}}
\newlabel{fig:weak convergence comparison set 3}{{7.7}{41}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error},using the ABR scheme as in Section \ref {sec:The ABR method}, for the European call option under the discretized Heston model, for Set $3$ parameters in Table \ref {table:Reference solution, for different parameter constellations.}. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.3}Conclusion about the chosen simulation scheme used for ASGQ}{41}{subsubsection.7.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}European call option under the discretized Heston model}{42}{subsection.7.6}}
\newlabel{sec:European call option under the discretized Heston model with parameters of Set 1}{{7.6}{42}{European call option under the discretized Heston model}{subsection.7.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{42}{table.caption.35}}
\newlabel{Total error of MISC and MC to compute European call option price under discretized Heston model of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{7.2}{42}{Total relative error of ASGQ, with different tolerances, and MC to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{42}{table.caption.36}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation}{{7.3}{42}{Comparison of the computational time of MC and ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces Total relative error of ASGQ and MC to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{43}{table.caption.37}}
\newlabel{Total relative error of ASGQ to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors.}{{7.4}{43}{Total relative error of ASGQ and MC to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.5}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{43}{table.caption.38}}
\newlabel{table:The computational time of ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, with level $1$ Richardson extrapolation.}{{7.5}{43}{Comparison of the computational time of MC and ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of European call option price under discretized Heston model. This plot shows that to achieve a relative error around $1.4\%$, using level $1$ of Richardson extrapolation is the optimal configuration for both methods, and that ASGQ coupled with level $1$ of Richardson extrapolation outperforms MC coupled with level $1$ of Richardson extrapolation method in terms of computational time.\relax }}{43}{figure.caption.39}}
\newlabel{fig:Complexity plot for MC and MISC , European call option price under discretized Heston model}{{7.8}{43}{Computational work comparison for ASGQ and MC methods, for the case of European call option price under discretized Heston model. This plot shows that to achieve a relative error around $1.4\%$, using level $1$ of Richardson extrapolation is the optimal configuration for both methods, and that ASGQ coupled with level $1$ of Richardson extrapolation outperforms MC coupled with level $1$ of Richardson extrapolation method in terms of computational time.\relax }{figure.caption.39}{}}
\citation{avikainen2009irregular}
\citation{giles2009analysing}
\citation{giles2008improved}
\citation{giles2013numerical}
\citation{giles2015multilevel}
\citation{giles2008improved}
\citation{giles2013numerical}
\citation{giles2015multilevel}
\@writefile{toc}{\contentsline {section}{\numberline {8}Numerical smoothing with MLMC}{44}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Our contributions to the literature}{44}{subsection.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}MLMC for digital options}{45}{subsection.8.2}}
\newlabel{sec: MLMC for digital options}{{8.2}{45}{MLMC for digital options}{subsection.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Numerical results for a digital call option using the MLMC method coupled with Euler-Maruyama discretisation of the GBM SDE, and without smoothing of the payoff.\relax }}{47}{figure.caption.40}}
\newlabel{fig:euler_digital_without_smoothing}{{8.1}{47}{Numerical results for a digital call option using the MLMC method coupled with Euler-Maruyama discretisation of the GBM SDE, and without smoothing of the payoff.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Numerical results for a digital call option using the MLMC method coupled with Euler-Maruyama discretisation of the GBM SDE, after applying the numerical smoothing to the payoff. We mention that observing a decaying variance of $P_{\ell }$ here is expected since we used Brownian bridge for path construction, and the QoI depends only on the terminal value of the Brownian bridge which has a variance scaled of order $\Delta t$.\relax }}{48}{figure.caption.41}}
\newlabel{fig:euler_digital_with_smoothing}{{8.2}{48}{Numerical results for a digital call option using the MLMC method coupled with Euler-Maruyama discretisation of the GBM SDE, after applying the numerical smoothing to the payoff. We mention that observing a decaying variance of $P_{\ell }$ here is expected since we used Brownian bridge for path construction, and the QoI depends only on the terminal value of the Brownian bridge which has a variance scaled of order $\Delta t$.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}MLMC for approximating densities and Greeks}{49}{subsection.8.3}}
\newlabel{sec: MLMC for approximating densities and greeks}{{8.3}{49}{MLMC for approximating densities and Greeks}{subsection.8.3}{}}
\bibstyle{plain}
\bibdata{smoothing}
\bibcite{acworth1998comparison}{1}
\bibcite{alfonsi2010high}{2}
\bibcite{andersen2007efficient}{3}
\bibcite{andersen2005extended}{4}
\bibcite{avikainen2009irregular}{5}
\bibcite{bayersmoothing}{6}
\bibcite{broadie2006exact}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Numerical results for density estimation using the MLMC method coupled with Euler-Maruyama discretisation of the GBM SDE, after applying the numerical smoothing. We mention that observing a decaying variance of $P_{\ell }$ here is expected since we used Brownian bridge for path construction, and the QoI depends only on the terminal value of the Brownian bridge which has a variance scaled of order $\Delta t$.\relax }}{50}{figure.caption.42}}
\newlabel{fig:euler_density_MLMC_with_smoothing}{{8.3}{50}{Numerical results for density estimation using the MLMC method coupled with Euler-Maruyama discretisation of the GBM SDE, after applying the numerical smoothing. We mention that observing a decaying variance of $P_{\ell }$ here is expected since we used Brownian bridge for path construction, and the QoI depends only on the terminal value of the Brownian bridge which has a variance scaled of order $\Delta t$.\relax }{figure.caption.42}{}}
\bibcite{bungartz2004sparse}{8}
\bibcite{caflisch1997valuation}{9}
\bibcite{giles2013numerical}{10}
\bibcite{giles2009analysing}{11}
\bibcite{giles2015multilevel}{12}
\bibcite{giles2008improved}{13}
\bibcite{glasserman2004monte}{14}
\bibcite{griebel2013smoothing}{15}
\bibcite{griebel2017note}{16}
\bibcite{griewank2017high}{17}
\bibcite{haji2016multi}{18}
\bibcite{heston1993closed}{19}
\bibcite{imai2004minimizing}{20}
\bibcite{jeanblanc2009mathematical}{21}
\bibcite{kahl2006fast}{22}
\bibcite{lord2010comparison}{23}
\bibcite{morokoff1994quasi}{24}
\bibcite{moskowitz1996smoothness}{25}
\bibcite{talay1990expansion}{26}
\bibcite{xiao2018conditional}{27}
