\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bungartz2004sparse}
\citation{griebel2013smoothing}
\citation{bayersmoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\citation{xiao2018conditional}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griebel2013smoothing}
\citation{griebel2017note}
\citation{griewank2017high}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{xiao2018conditional}
\citation{bayersmoothing}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem formulation and Setting}{2}{section.2}}
\newlabel{sec:General setting}{{2}{2}{Problem formulation and Setting}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Continuous time formulation}{2}{subsection.2.1}}
\newlabel{eq:SDE_interest}{{2.1}{2}{Continuous time formulation}{equation.2.1}{}}
\newlabel{eq:smoothing_decomposition}{{2.3}{2}{Continuous time formulation}{equation.2.3}{}}
\newlabel{eq:smoothing_decomposition_componentwise}{{2.4}{2}{Continuous time formulation}{equation.2.4}{}}
\newlabel{eq:SDE_decomposition_componentwise_exapanded}{{2.6}{3}{Continuous time formulation}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}First approach (Brutal)}{3}{subsubsection.2.1.1}}
\newlabel{eq: variance global terms}{{2.7}{3}{First approach (Brutal)}{equation.2.7}{}}
\newlabel{eq:approximate_dynamics}{{2.8}{4}{First approach (Brutal)}{equation.2.8}{}}
\newlabel{eq:first_moment_approximation}{{2.10}{4}{First approach (Brutal)}{equation.2.10}{}}
\newlabel{eq:second_moment_approximation}{{2.11}{4}{First approach (Brutal)}{equation.2.11}{}}
\newlabel{eq:covariance_dynamcis_approximation}{{2.12}{5}{First approach (Brutal)}{equation.2.12}{}}
\newlabel{eq:call_option}{{2.13}{5}{First approach (Brutal)}{equation.2.13}{}}
\newlabel{eq:binary_option}{{2.14}{5}{First approach (Brutal)}{equation.2.14}{}}
\newlabel{assump:Monotonicity condition}{{2.15}{6}{First approach (Brutal)}{equation.2.15}{}}
\newlabel{assump:Growth condition}{{2.16}{6}{First approach (Brutal)}{equation.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Discrete time formulation}{6}{subsection.2.2}}
\newlabel{sec:Discrete time, practical motivation}{{2.2}{6}{Discrete time formulation}{subsection.2.2}{}}
\newlabel{lognormal_dynamics_basket}{{2.18}{6}{Discrete time formulation}{equation.2.18}{}}
\newlabel{eq: option price as integral_basket}{{2.19}{6}{Discrete time formulation}{equation.2.19}{}}
\newlabel{eq:discrete_rep}{{2.20}{7}{Discrete time formulation}{equation.2.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Step $1$: Numerical smoothing}{7}{subsubsection.2.2.1}}
\newlabel{sec:Step $1$: Numerical smoothing}{{2.2.1}{7}{Step $1$: Numerical smoothing}{subsubsection.2.2.1}{}}
\newlabel{eq:linear_transformation}{{2.21}{7}{Step $1$: Numerical smoothing}{equation.2.21}{}}
\newlabel{eq:discrete_rep_2}{{2.22}{7}{Step $1$: Numerical smoothing}{equation.2.22}{}}
\newlabel{eq: incremental functions}{{2.23}{7}{Step $1$: Numerical smoothing}{equation.2.23}{}}
\newlabel{polynomial_kink_location_basket}{{2.25}{7}{Step $1$: Numerical smoothing}{equation.2.25}{}}
\newlabel{polynomial_kink_location_derivative_basket}{{2.26}{7}{Step $1$: Numerical smoothing}{equation.2.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Step $2$: Integration}{8}{subsubsection.2.2.2}}
\newlabel{sec:Step $2$: Integration}{{2.2.2}{8}{Step $2$: Integration}{subsubsection.2.2.2}{}}
\newlabel{eq: pre_integration_step_wrt_y1_basket}{{2.27}{8}{Step $2$: Integration}{equation.2.27}{}}
\newlabel{eq:smooth_function_after_pre_integration}{{2.28}{8}{Step $2$: Integration}{equation.2.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analyticity and Smoothness Analysis}{8}{section.3}}
\newlabel{sec:Analiticity Analysis}{{3}{8}{Analyticity and Smoothness Analysis}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Haar construction of Brownian motion revisited}{8}{subsection.3.1}}
\newlabel{sec:haar-constr-brown}{{3.1}{8}{Haar construction of Brownian motion revisited}{subsection.3.1}{}}
\newlabel{eq:Haar-mother}{{3.1}{9}{Haar construction of Brownian motion revisited}{equation.3.1}{}}
\newlabel{eq:Haar-basis}{{3.2}{9}{Haar construction of Brownian motion revisited}{equation.3.2}{}}
\newlabel{eq:Haar-constant}{{3.2a}{9}{Haar construction of Brownian motion revisited}{equation.3.2a}{}}
\newlabel{eq:Haar-nonconstant}{{3.2b}{9}{Haar construction of Brownian motion revisited}{equation.3.2b}{}}
\newlabel{eq:Haar-int-basis}{{3.3}{9}{Haar construction of Brownian motion revisited}{equation.3.3}{}}
\newlabel{eq:Haar-int-constant}{{3.3a}{9}{Haar construction of Brownian motion revisited}{equation.3.3a}{}}
\newlabel{eq:Haar-int-nonconstant}{{3.3b}{9}{Haar construction of Brownian motion revisited}{equation.3.3b}{}}
\newlabel{eq:Brownian-motion}{{3.4}{9}{Haar construction of Brownian motion revisited}{equation.3.4}{}}
\newlabel{eq:Brownian-motion-truncated}{{3.5}{9}{Haar construction of Brownian motion revisited}{equation.3.5}{}}
\newlabel{eq:increments}{{3.6}{9}{Haar construction of Brownian motion revisited}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Stochastic differential equations}{9}{subsection.3.2}}
\newlabel{sec:stoch-diff-equat}{{3.2}{9}{Stochastic differential equations}{subsection.3.2}{}}
\newlabel{eq:SDE}{{3.7}{9}{Stochastic differential equations}{equation.3.7}{}}
\newlabel{eq:euler}{{3.8}{10}{Stochastic differential equations}{equation.3.8}{}}
\newlabel{eq:H-function}{{3.9}{10}{Stochastic differential equations}{equation.3.9}{}}
\newlabel{eq:1}{{3.10}{10}{Stochastic differential equations}{equation.3.10}{}}
\newlabel{ass:boundedness-derivative}{{3.1}{11}{}{theorem.3.1}{}}
\newlabel{ass:boundedness-inverse}{{3.3}{11}{}{theorem.3.3}{}}
\newlabel{lem:dXdZ}{{3.4}{11}{}{theorem.3.4}{}}
\newlabel{eq:dWdZ}{{3.11}{11}{Stochastic differential equations}{equation.3.11}{}}
\newlabel{eq:2}{{3.12}{11}{Stochastic differential equations}{equation.3.12}{}}
\newlabel{lem:d2XdZdY}{{3.5}{12}{}{theorem.3.5}{}}
\newlabel{eq:d2XdWdW}{{3.13}{12}{Stochastic differential equations}{equation.3.13}{}}
\newlabel{prop:first-derivatives}{{3.7}{12}{}{theorem.3.7}{}}
\newlabel{lem:d2XdZ2}{{3.8}{13}{}{theorem.3.8}{}}
\newlabel{thr:smoothness}{{3.9}{13}{}{theorem.3.9}{}}
\newlabel{rem:analyticity}{{3.10}{13}{}{theorem.3.10}{}}
\citation{haji2016multi}
\@writefile{toc}{\contentsline {section}{\numberline {4}Details of our hierarchical method}{14}{section.4}}
\newlabel{sec:Details of our approach}{{4}{14}{Details of our hierarchical method}{section.4}{}}
\newlabel{eq:total_error}{{4.1}{14}{Details of our hierarchical method}{equation.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Adaptive Sparse Grids}{14}{subsection.4.1}}
\newlabel{sec:Details of the ASGQ}{{4.1}{14}{Adaptive Sparse Grids}{subsection.4.1}{}}
\citation{bungartz2004sparse}
\citation{morokoff1994quasi}
\citation{moskowitz1996smoothness}
\citation{caflisch1997valuation}
\citation{acworth1998comparison}
\citation{imai2004minimizing}
\citation{glasserman2004monte}
\newlabel{eq:MISC_quad_estimator}{{4.2}{16}{Adaptive Sparse Grids}{equation.4.2}{}}
\newlabel{eq:quadrature error}{{4.3}{16}{Adaptive Sparse Grids}{equation.4.3}{}}
\newlabel{eq:Work_error_contributions}{{4.4}{16}{Adaptive Sparse Grids}{equation.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Brownian bridge (Bb) construction}{16}{subsection.4.2}}
\newlabel{sec:Brwonian bridge construction}{{4.2}{16}{Brownian bridge (Bb) construction}{subsection.4.2}{}}
\citation{talay1990expansion}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Richardson extrapolation}{17}{subsection.4.3}}
\newlabel{sec:Richardson extrapolation}{{4.3}{17}{Richardson extrapolation}{subsection.4.3}{}}
\newlabel{Euler_weak_error_strenghten}{{4.5}{17}{Richardson extrapolation}{equation.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Root Finding}{17}{subsection.4.4}}
\newlabel{sec: Root Finding}{{4.4}{17}{Root Finding}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Error discussion}{18}{section.5}}
\newlabel{sec:Error discussion}{{5}{18}{Error discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Errors in smoothing}{18}{subsection.5.1}}
\newlabel{sec:errors-smoothing}{{5.1}{18}{Errors in smoothing}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical experiments}{19}{section.6}}
\newlabel{optimal_number_samples}{{6.1}{19}{Numerical experiments}{Item.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of relative errors and computational gains, achieved by the different methods. In this table, we highlight the computational gains achieved by ASGQ over MC method to meet a certain error tolerance. We provide details about the way we compute these gains for each case in the following sections.\relax }}{20}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:Summary of our numerical results.}{{6.1}{20}{Summary of relative errors and computational gains, achieved by the different methods. In this table, we highlight the computational gains achieved by ASGQ over MC method to meet a certain error tolerance. We provide details about the way we compute these gains for each case in the following sections.\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Options under the discretized one dimensional GBM model}{20}{subsection.6.1}}
\newlabel{sec:The discretized 1D Black-Scholes}{{6.1}{20}{Options under the discretized one dimensional GBM model}{subsection.6.1}{}}
\newlabel{lognormal_dynamics}{{6.2}{20}{Options under the discretized one dimensional GBM model}{equation.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Determining the kink location}{21}{subsubsection.6.1.1}}
\newlabel{sec:Determining the kink location}{{6.1.1}{21}{Determining the kink location}{subsubsection.6.1.1}{}}
\newlabel{eq: kink_point_problem}{{6.4}{21}{Exact location of the kink for the continuous problem}{equation.6.4}{}}
\newlabel{xact_location_continuous_problem}{{6.6}{21}{Exact location of the kink for the continuous problem}{equation.6.6}{}}
\newlabel{polynomial_kink_location}{{6.10}{21}{Location of the kink for the discrete problem}{equation.6.10}{}}
\newlabel{polynomial_kink_location_derivative}{{6.11}{22}{Location of the kink for the discrete problem}{equation.6.11}{}}
\newlabel{smoothed_integrand_single_opt_1d}{{6.12}{22}{Location of the kink for the discrete problem}{equation.6.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Results for the single binary option under discretized GBM model}{22}{subsubsection.6.1.2}}
\newlabel{sec:Results for the binary option example}{{6.1.2}{22}{Results for the single binary option under discretized GBM model}{subsubsection.6.1.2}{}}
\newlabel{smoothed_integrand_binary_opt_2}{{6.14}{22}{Results for the single binary option under discretized GBM model}{equation.6.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, using MC with $M=10^4$ samples for the binary option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{23}{figure.caption.4}}
\newlabel{fig:Weak_rate_binary}{{6.1}{23}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, using MC with $M=10^4$ samples for the binary option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute binary option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{23}{table.caption.5}}
\newlabel{Total error of MISC and MC to compute Binary option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.2}{23}{Total relative error of ASGQ, with different tolerances, and MC to compute binary option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute binary option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{23}{table.caption.6}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute Binary option price for different number of time steps, without Richardson extrapolation}{{6.3}{23}{Comparison of the computational time of MC and ASGQ, used to compute binary option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of binary option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms MC method in terms of computational time\relax }}{24}{figure.caption.7}}
\newlabel{fig:Complexity plot for MC and MISC , Binary, Non rich}{{6.2}{24}{Computational work comparison for ASGQ and MC methods, for the case of binary option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms MC method in terms of computational time\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Results for the single call option example}{24}{subsubsection.6.1.3}}
\newlabel{sec:Results for the call option example}{{6.1.3}{24}{Results for the single call option example}{subsubsection.6.1.3}{}}
\newlabel{smoothed_integrand_call_opt_2}{{6.16}{24}{Results for the single call option example}{equation.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, using MC with $M=4 \times 10^5$ samples for the call option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{25}{figure.caption.8}}
\newlabel{fig:Weak_rate_call_beta_32}{{6.3}{25}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, using MC with $M=4 \times 10^5$ samples for the call option example. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{25}{table.caption.9}}
\newlabel{Total error of MISC and MC to compute Call option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.4}{25}{Total relative error of ASGQ, with different tolerances, and MC to compute call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{25}{table.caption.10}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute Call option price for different number of time steps, without Richardson extrapolation}{{6.5}{25}{Comparison of the computational time of MC and ASGQ, used to compute call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }}{26}{figure.caption.11}}
\newlabel{fig:Complexity plot for MC and MISC , Call non rich}{{6.4}{26}{Computational work comparison for ASGQ and MC methods, for the case of call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}The basket call option under GBM model}{26}{subsection.6.2}}
\newlabel{sec:The basket call option under GBM model}{{6.2}{26}{The basket call option under GBM model}{subsection.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}$d=2$}{26}{subsection.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, for the two dimensional basket call option with a number of Laguerre quadrature points $\beta =128$ and number of samples for MC $M=10^7$. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{26}{figure.caption.12}}
\newlabel{fig:Weak_rate_two_dim_basket}{{6.5}{26}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, for the two dimensional basket call option with a number of Laguerre quadrature points $\beta =128$ and number of samples for MC $M=10^7$. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{27}{table.caption.13}}
\newlabel{Total error of MISC and MC to compute two dim basket Call option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.6}{27}{Total relative error of ASGQ, with different tolerances, and MC to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{27}{table.caption.14}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute two dim basket Call option price for different number of time steps, without Richardson extrapolation}{{6.7}{27}{Comparison of the computational time of MC and ASGQ, used to compute two dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of two dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }}{27}{figure.caption.15}}
\newlabel{fig:Complexity plot for MC and MISC , two dim basket call non rich}{{6.6}{27}{Computational work comparison for ASGQ and MC methods, for the case of two dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}$d=4$}{28}{subsection.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, for the $4$-dimensional basket call option with a number of Laguerre quadrature points $\beta =512$ and number of samples for MC $M=4 \times 10^6$. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{28}{figure.caption.16}}
\newlabel{fig:Weak_rate_4_dim_basket}{{6.7}{28}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, for the $4$-dimensional basket call option with a number of Laguerre quadrature points $\beta =512$ and number of samples for MC $M=4 \times 10^6$. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.8}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute $4$-dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{28}{table.caption.17}}
\newlabel{Total error of MISC and MC to compute 4 dim basket Call option price of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.8}{28}{Total relative error of ASGQ, with different tolerances, and MC to compute $4$-dimensional basket call option price for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.17}{}}
\citation{heston1993closed}
\citation{broadie2006exact}
\citation{kahl2006fast}
\citation{andersen2007efficient}
\citation{lord2010comparison}
\@writefile{lot}{\contentsline {table}{\numberline {6.9}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute $4$-dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{29}{table.caption.18}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute 4 dim basket Call option price for different number of time steps, without Richardson extrapolation}{{6.9}{29}{Comparison of the computational time of MC and ASGQ, used to compute $4$-dimensional basket call option price for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of $4$-dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }}{29}{figure.caption.19}}
\newlabel{fig:Complexity plot for MC and MISC , 4 dim basket call non rich}{{6.8}{29}{Computational work comparison for ASGQ and MC methods, for the case of $4$-dimensional basket call option. This plot shows that to achieve a relative error below $1\%$, ASGQ outperforms significantly MC method in terms of computational time.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Options under the discretized the Heston model}{29}{subsection.6.5}}
\newlabel{eq:dynamics Heston}{{6.18}{29}{Options under the discretized the Heston model}{equation.6.18}{}}
\citation{lord2010comparison}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}Discretization of Heston model with a non smooth transformations for the volatility process}{30}{subsubsection.6.5.1}}
\newlabel{sec:Discretization of Heston model with a non smooth transformations for the volatility process}{{6.5.1}{30}{Discretization of Heston model with a non smooth transformations for the volatility process}{subsubsection.6.5.1}{}}
\newlabel{eq:FE_Heston_discrete}{{6.19}{30}{Discretization of Heston model with a non smooth transformations for the volatility process}{equation.6.19}{}}
\newlabel{Numerical schemes for CIR process}{{\caption@xref {Numerical schemes for CIR process}{ on input line 1247}}{30}{Discretization of Heston model with a non smooth transformations for the volatility process}{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2}Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{30}{subsubsection.6.5.2}}
\newlabel{sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{{6.5.2}{30}{Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{subsubsection.6.5.2}{}}
\newlabel{equivalent OU process}{{6.20}{30}{Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{equation.6.20}{}}
\citation{jeanblanc2009mathematical}
\citation{haji2016multi}
\newlabel{eq:expressing CIR processes from OU processes}{{6.23}{31}{Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{equation.6.23}{}}
\newlabel{eq:equivalent CIR process}{{6.27}{31}{Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}{equation.6.27}{}}
\citation{heston1993closed}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.3}Comparison between the two approaches of simulating the volatility process ($n=1$)}{32}{subsubsection.6.5.3}}
\newlabel{sec:Comparison between the two approaches of simulating the volatlity process}{{6.5.3}{32}{Comparison between the two approaches of simulating the volatility process ($n=1$)}{subsubsection.6.5.3}{}}
\newlabel{fig:sub3}{{6.9a}{32}{\relax }{figure.caption.21}{}}
\newlabel{sub@fig:sub3}{{a}{32}{\relax }{figure.caption.21}{}}
\newlabel{fig:sub4}{{6.9b}{32}{\relax }{figure.caption.21}{}}
\newlabel{sub@fig:sub4}{{b}{32}{\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces The rate of error convergence of first order differences $\left \delimiter 69640972 \Delta \text  {E}_{\boldsymbol  {\beta }}\right \delimiter 86418188 $, defined by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:Work_error_contributions}\unskip \@@italiccorr )}}, ($\boldsymbol  {\beta }=\mathbf  {1}+k \overline  {\boldsymbol  {\beta }}$) for the example of single call option under Heston model using $N=2$ time steps. In this case, the first dimension is used for the volatility noise (mainly $Z_v$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:FE_Heston_discrete}\unskip \@@italiccorr )}}) and the final coordinate is related to $Z_t$ (used to get the noise related to the asset dynamics) (a) using full truncation as in Section \ref  {sec:Discretization of Heston model with a non smooth transformations for the volatility process} (b) using the smooth transformation as in Section \ref  {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}. \relax }}{32}{figure.caption.21}}
\newlabel{fig:first_diff_Heston_call_N_2}{{6.9}{32}{The rate of error convergence of first order differences $\abs {\Delta \text {E}_{\boldsymbol {\beta }}}$, defined by \eqref {eq:Work_error_contributions}, ($\boldsymbol {\beta }=\mathbf {1}+k \bar {\boldsymbol {\beta }}$) for the example of single call option under Heston model using $N=2$ time steps. In this case, the first dimension is used for the volatility noise (mainly $Z_v$ in \eqref {eq:FE_Heston_discrete}) and the final coordinate is related to $Z_t$ (used to get the noise related to the asset dynamics) (a) using full truncation as in Section \ref {sec:Discretization of Heston model with a non smooth transformations for the volatility process} (b) using the smooth transformation as in Section \ref {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}. \relax }{figure.caption.21}{}}
\citation{heston1993closed}
\newlabel{fig:sub3}{{6.10a}{33}{\relax }{figure.caption.22}{}}
\newlabel{sub@fig:sub3}{{a}{33}{\relax }{figure.caption.22}{}}
\newlabel{fig:sub4}{{6.10b}{33}{\relax }{figure.caption.22}{}}
\newlabel{sub@fig:sub4}{{b}{33}{\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces The rate of error convergence of first order differences $\left \delimiter 69640972 \Delta \text  {E}_{\boldsymbol  {\beta }}\right \delimiter 86418188 $, defined by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:Work_error_contributions}\unskip \@@italiccorr )}}, ($\boldsymbol  {\beta }=\mathbf  {1}+k \overline  {\boldsymbol  {\beta }}$) for the example of single call option under Heston model using $N=4$ time steps. In this case, we just show the first $4$ dimensions which are used for the volatility noise (mainly $Z_v$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:FE_Heston_discrete}\unskip \@@italiccorr )}}). (a) using full truncation as in Section \ref  {sec:Discretization of Heston model with a non smooth transformations for the volatility process} (b) using the smooth transformation as in Section \ref  {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}. \relax }}{33}{figure.caption.22}}
\newlabel{fig:first_diff_Heston_call_N_4}{{6.10}{33}{The rate of error convergence of first order differences $\abs {\Delta \text {E}_{\boldsymbol {\beta }}}$, defined by \eqref {eq:Work_error_contributions}, ($\boldsymbol {\beta }=\mathbf {1}+k \bar {\boldsymbol {\beta }}$) for the example of single call option under Heston model using $N=4$ time steps. In this case, we just show the first $4$ dimensions which are used for the volatility noise (mainly $Z_v$ in \eqref {eq:FE_Heston_discrete}). (a) using full truncation as in Section \ref {sec:Discretization of Heston model with a non smooth transformations for the volatility process} (b) using the smooth transformation as in Section \ref {sec:Discretization of Heston model with the volatility process Simulated using the sum of Ornstein-Uhlenbeck (Bessel) processes}. \relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.4}European call option under the discretized Heston model ($n=1$) }{33}{subsubsection.6.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces The convergence of the relative weak error $\mathcal  {E}_B(N)$ defined in \ref  {eq:total_error}, for the European call option under the discretized Heston model, with a number of Laguerre quadrature points $\beta =512$ and number of samples for MC $M=4 \times 10^6$. The upper and lower bounds are $95\%$ confidence intervals.\relax }}{34}{figure.caption.23}}
\newlabel{fig:Weak_rate_call_heston_smooth_vol}{{6.11}{34}{The convergence of the relative weak error $\mathcal {E}_B(N)$ defined in \ref {eq:total_error}, for the European call option under the discretized Heston model, with a number of Laguerre quadrature points $\beta =512$ and number of samples for MC $M=4 \times 10^6$. The upper and lower bounds are $95\%$ confidence intervals.\relax }{figure.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.10}{\ignorespaces Total relative error of ASGQ, with different tolerances, and MC to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{34}{table.caption.24}}
\newlabel{Total error of MISC and MC to compute European call option price under discretized Heston model of the different tolerances for different number of time steps, without Richardson extrapolation. The numbers between parentheses are the corresponding absolute errors.}{{6.10}{34}{Total relative error of ASGQ, with different tolerances, and MC to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.11}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{34}{table.caption.25}}
\newlabel{Comparsion of the computational time of MC and MISC, used to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation}{{6.11}{34}{Comparison of the computational time of MC and ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, without Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.25}{}}
\bibstyle{plain}
\bibdata{smoothing}
\bibcite{acworth1998comparison}{1}
\@writefile{lot}{\contentsline {table}{\numberline {6.12}{\ignorespaces Total relative error of ASGQ and MC to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {optimal_number_samples}\unskip \@@italiccorr )}}.\relax }}{35}{table.caption.26}}
\newlabel{Total relative error of ASGQ to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors.}{{6.12}{35}{Total relative error of ASGQ and MC to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The values between parentheses correspond to the different errors contributing to the total relative error: for ASGQ we report the bias and quadrature errors and for MC we report the bias and the statistical errors. The number of MC samples,$ M$, is chosen to satisfy \eqref {optimal_number_samples}.\relax }{table.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.13}{\ignorespaces Comparison of the computational time of MC and ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }}{35}{table.caption.27}}
\newlabel{table:The computational time of ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, with level $1$ Richardson extrapolation.}{{6.13}{35}{Comparison of the computational time of MC and ASGQ, used to compute European call option price under discretized Heston model for different number of time steps, with level $1$ of Richardson extrapolation. The average computational time of MC is computed over $10$ runs.\relax }{table.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Computational work comparison for ASGQ and MC methods, for the case of European call option price under discretized Heston model. This plot shows that to achieve a relative error around $1.4\%$, using level $1$ of Richardson extrapolation is the optimal configuration for both methods, and that ASGQ coupled with level $1$ of Richardson extrapolation outperforms MC coupled with level $1$ of Richardson extrapolation method in terms of computational time.\relax }}{35}{figure.caption.28}}
\newlabel{fig:Complexity plot for MC and MISC , European call option price under discretized Heston model}{{6.12}{35}{Computational work comparison for ASGQ and MC methods, for the case of European call option price under discretized Heston model. This plot shows that to achieve a relative error around $1.4\%$, using level $1$ of Richardson extrapolation is the optimal configuration for both methods, and that ASGQ coupled with level $1$ of Richardson extrapolation outperforms MC coupled with level $1$ of Richardson extrapolation method in terms of computational time.\relax }{figure.caption.28}{}}
\bibcite{andersen2007efficient}{2}
\bibcite{bayersmoothing}{3}
\bibcite{broadie2006exact}{4}
\bibcite{bungartz2004sparse}{5}
\bibcite{caflisch1997valuation}{6}
\bibcite{glasserman2004monte}{7}
\bibcite{griebel2013smoothing}{8}
\bibcite{griebel2017note}{9}
\bibcite{griewank2017high}{10}
\bibcite{haji2016multi}{11}
\bibcite{heston1993closed}{12}
\bibcite{imai2004minimizing}{13}
\bibcite{jeanblanc2009mathematical}{14}
\bibcite{kahl2006fast}{15}
\bibcite{lord2010comparison}{16}
\bibcite{morokoff1994quasi}{17}
\bibcite{moskowitz1996smoothness}{18}
\bibcite{talay1990expansion}{19}
\bibcite{xiao2018conditional}{20}
